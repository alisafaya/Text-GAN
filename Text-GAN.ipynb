{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-GAN Turkish word generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readwordset (generic function with 1 method)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet, Test, Base.Iterators, Printf, LinearAlgebra, CuArrays, Random, IterTools\n",
    "\n",
    "struct Charset\n",
    "    c2i::Dict{Any,Int}\n",
    "    i2c::Vector{Any}\n",
    "    eow::Int\n",
    "end\n",
    "\n",
    "function Charset(charset::String; eow=\"\")\n",
    "    i2c = [ eow; [ c for c in charset ]  ]\n",
    "    print(i2c)\n",
    "    c2i = Dict( c => i for (i, c) in enumerate(i2c))\n",
    "    return Charset(c2i, i2c, c2i[eow])\n",
    "end\n",
    "\n",
    "struct TextReader\n",
    "    file::String\n",
    "    charset::Charset\n",
    "end\n",
    "\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    s === nothing && (s = open(r.file))\n",
    "    eof(s) && return close(s)\n",
    "    return [ get(r.charset.c2i, c, r.charset.eow) for c in readline(s)], s\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}\n",
    "\n",
    "struct WordsData\n",
    "    src::TextReader        \n",
    "    batchsize::Int         \n",
    "    maxlength::Int         \n",
    "    batchmajor::Bool       \n",
    "    bucketwidth::Int    \n",
    "    buckets::Vector        \n",
    "    batchmaker::Function   \n",
    "end\n",
    "\n",
    "function WordsData(src::TextReader; batchmaker = arraybatch, batchsize = 128, maxlength = typemax(Int),\n",
    "                batchmajor = false, bucketwidth = 2, numbuckets = min(128, maxlength ÷ bucketwidth))\n",
    "    buckets = [ [] for i in 1:numbuckets ] # buckets[i] is an array of sentence pairs with similar length\n",
    "    WordsData(src, batchsize, maxlength, batchmajor, bucketwidth, buckets, batchmaker)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{WordsData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{WordsData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{WordsData}) = NTuple{2}\n",
    "\n",
    "function Base.iterate(d::WordsData, state=nothing)\n",
    "    if state == 0 # When file is finished but buckets are partially full \n",
    "        for i in 1:length(d.buckets)\n",
    "            if length(d.buckets[i]) > 0\n",
    "                buc = d.buckets[i]\n",
    "                d.buckets[i] = []\n",
    "                return buc, state\n",
    "            end\n",
    "        end\n",
    "        return nothing # Finish iteration\n",
    "    end\n",
    "\n",
    "    while true\n",
    "        src_next = iterate(d.src, state)\n",
    "        \n",
    "        if src_next === nothing\n",
    "            state = 0\n",
    "            return iterate(d, state)\n",
    "        end\n",
    "        \n",
    "        (src_word, src_state) = src_next\n",
    "        state = src_state\n",
    "        src_length = length(src_word)\n",
    "        \n",
    "        (src_length > d.maxlength) && continue\n",
    "\n",
    "        i = Int(ceil(src_length / d.bucketwidth))\n",
    "        i > length(d.buckets) && (i = length(d.buckets))\n",
    "\n",
    "        push!(d.buckets[i], src_word)\n",
    "        if length(d.buckets[i]) == d.batchsize\n",
    "            buc = d.buckets[i]\n",
    "            d.buckets[i] = []\n",
    "            return buc, state\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# function arraybatch(d::WordsData, bucket)\n",
    "#     src_eow = d.src.charset.eow\n",
    "#     src_lengths = map(x -> length(x), bucket)\n",
    "#     max_length = max(src_lengths...)\n",
    "#     x = zeros(Int64, length(bucket), max_length + 2) # default d.batchmajor is false\n",
    "\n",
    "#     for (i, v) in enumerate(bucket)\n",
    "#         to_be_added = fill(src_eow, max_length - length(v) + 1)\n",
    "#         x[i,:] = [src_eow; v; to_be_added]\n",
    "#     end\n",
    "\n",
    "#     d.batchmajor && (x = x')\n",
    "#     return (x[:, 1:end-1], x[:, 2:end])\n",
    "# end\n",
    "\n",
    "function readwordset(fname)\n",
    "    words = []\n",
    "    fi = open(fname)\n",
    "    while !eof(fi)\n",
    "        push!(words, readline(fi))\n",
    "    end\n",
    "    close(fi)\n",
    "    words\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G/D/S Common Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embed"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Embed; w; end\n",
    "\n",
    "function Embed(shape...)\n",
    "    Embed(param(shape...))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate (generic function with 1 method)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_z(shape...) = KnetArray(randn(Float32, shape...))\n",
    "\n",
    "\n",
    "### Not used \n",
    "# concatenate z with embedding vectors, z -> (z_size, B), returns (E+z_size, B, T)\n",
    "# this will be used to feed Z to generator at each timestep\n",
    "# function (l::Embed)(x, z)\n",
    "#     em = l.w[:, x]\n",
    "#     z_array = cat((z for i in 1:size(em, 3))...; dims=(3))\n",
    "#     cat(em, z_array; dims=(1))\n",
    "# end\n",
    "\n",
    "# Generator model\n",
    "struct GenModel\n",
    "    projection::Embed\n",
    "    rnn::RNN        \n",
    "    dropout::Real\n",
    "    charset::Charset \n",
    "end\n",
    "\n",
    "function GenModel(hidden::Int, charset::Charset; layers=2, dropout=0)\n",
    "    rnn = RNN(1, hidden; numLayers=layers, dropout=dropout) # input size is 1\n",
    "    projection = Embed(hidden, length(charset.i2c))\n",
    "    GenModel(projection, rnn, dropout, charset)\n",
    "end\n",
    "\n",
    "# Generator forward pass, here Z is our latent var -> (H, Tx, )\n",
    "function (s::GenModel)(timesteps, batchsize)\n",
    "    s.rnn.h = get_z(s.rnn.hiddenSize, batchsize, s.rnn.numLayers) # according to get_z(H, B, layers)\n",
    "    s.rnn.c = get_z(s.rnn.hiddenSize, batchsize, s.rnn.numLayers) # according to get_z(H, B, layers)\n",
    "    rnn_out = s.rnn(KnetArray(ones(Float32, (1, batchsize, timesteps))))\n",
    "    dims = size(rnn_out)\n",
    "    output = s.projection.w' * dropout(reshape(rnn_out, dims[1], dims[2] * dims[3]), s.dropout)\n",
    "    reshape(softmax(output), size(output, 1), dims[2], dims[3])\n",
    "end\n",
    "\n",
    "function generate(s::GenModel, maxlength, batchsize)\n",
    "    out = s(maxlength, batchsize)\n",
    "    words = []\n",
    "    for i in 1:batchsize\n",
    "        push!(words, join([s.charset.i2c[x[1]] for x in argmax(out[:, i, :]; dims=1)], \"\"))\n",
    "    end\n",
    "    words\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: word sampler will be used to train discriminator. \n",
    "this sampler should take B, T (batchsize, timestep) as parameters\n",
    "returns (X, Y) tuple \n",
    "where X is tensor of size (C, B, T)\n",
    "and Y is array of size B\n",
    "B consists of real words and generated words\n",
    "C charset size where each value is weight of this char\n",
    "in the case of generated words the generator already gives C, B, T\n",
    "for real words we need to convert words to C, T arrays\n",
    "where every character can be represented by one hot vector or by Gumble-Max (which is normalized one hot vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Sampler\n",
    "    wordsdata::WordsData\n",
    "    charset::Charset\n",
    "    genModel::GenModel\n",
    "    maxBatchsize::Int\n",
    "end\n",
    "\n",
    "# this function is similar to gumble softmax, it is used to soften the one-hot-vector of the real samples\n",
    "# tau -> normalization factor; the bigger the softer\n",
    "function soften(A; dims=1, tau=2.0) \n",
    "    A = A ./ tau\n",
    "    softmax(A; dims=dims)\n",
    "end\n",
    "\n",
    "function Base.iterate(s::Sampler, state=nothing)\n",
    "    (bucket, state) = iterate(s.wordsdata, state)\n",
    "    bsize = length(bucket)\n",
    "    src_eow = d.charset.eow\n",
    "    src_lengths = map(x -> length(x), bucket)\n",
    "    max_length = max(src_lengths...)\n",
    "    gsize += (rand(Int) % (maxBatchsize - bsize)) # count of words to be generated\n",
    "    generated = generate(s.gmodel, max_length, gsize)\n",
    "\n",
    "    to_be_cat = []\n",
    "    for (i, v) in enumerate(bucket)\n",
    "        to_be_added = fill(src_eow, max_length - length(v))\n",
    "        cindex = [s.charset.c2i[c] for c in v]\n",
    "        tindex = [i for i in 1:length(v)]\n",
    "        onehot = KnetArray(zeros(length(s.charset.c2i), 1, max_length))\n",
    "        onehot[cindex, :, tindex] .= 1\n",
    "        push!(to_be_cat, onehot)\n",
    "    end\n",
    "    x = cat(generated, to_be_cat...;dims=2)\n",
    "    y = [0]\n",
    "    return (x,y), state\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y', 'z', 'Ç', 'Ö', 'Ü', 'ç', 'ö', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş']"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: arraybatch not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: arraybatch not defined",
      "",
      "Stacktrace:",
      " [1] (::getfield(Core, Symbol(\"#kw#Type\")))(::NamedTuple{(:batchsize, :maxlength, :bucketwidth),Tuple{Int64,Int64,Int64}}, ::Type{WordsData}, ::TextReader) at ./none:0",
      " [2] top-level scope at In[61]:20"
     ]
    }
   ],
   "source": [
    "char_set = \"ABCDEFGHIJKLMNOPRSTUVYZabcdefghijklmnoprstuvyzÇÖÜçöüĞğİıŞş\"\n",
    "tr_charset = Charset(char_set)\n",
    "datadir = \"turkish_word_set\"\n",
    "embedding_size = 64\n",
    "gmodel = GenModel(embedding_size, tr_charset; dropout=0.2)\n",
    "filter_no = 20\n",
    "ddrop = 0.2\n",
    "# dismodel = DisModel(tr_charset, gmodel.projection, (\n",
    "#         Conv(2,embedding_size,1,filter_no; pdrop=ddrop),\n",
    "#         Conv(3,embedding_size,1,filter_no; pdrop=ddrop),\n",
    "#         Conv(4,embedding_size,1,filter_no; pdrop=ddrop),\n",
    "#         ),(\n",
    "#         Dense(60,64,pdrop=0.3),\n",
    "#         Dense(64,2,sigm,pdrop=0.3)\n",
    "#         ))\n",
    "\n",
    "BATCHSIZE = 32\n",
    "MAXLENGTH = 25\n",
    "tr_dev = TextReader(\"$datadir/dev.tr\", tr_charset)\n",
    "dtrn = WordsData(tr_dev, batchsize=BATCHSIZE, maxlength=MAXLENGTH, bucketwidth = 1)\n",
    "sampler = Sampler(dtrn, tr_charset, gmodel, 32)\n",
    "\n",
    "iterate(sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This one to be used by DModel, takes weights of characters and reduce the embedding for each character\n",
    "# this approach to avoid sampling or argmaxing over rnn's output\n",
    "# (C, B, T) -> (T, E, 1, B)\n",
    "function (l::Embed)(x)\n",
    "    dims = size(out)\n",
    "    em = l.w * reshape(x, dims[1], dims[2] * dims[3]) # reshape for multiplication \n",
    "    em = reshape(em, size(em, 1), dims[2], dims[3]) # reshape to original size\n",
    "    em = permutedims(em, [3, 1, 2])  # permute for CONV\n",
    "    em = reshape(em, dims[3], size(em, 2), 1, dims[2]) # Add one dim for CONV\n",
    "end\n",
    "\n",
    "struct Conv; w; b; f; p; end\n",
    "(c::Conv)(x) = (co=conv4(c.w, dropout(x,c.p)); c.f.(pool((co .+ c.b); window=(size(co, 1), size(co, 2)))))\n",
    "Conv(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), f, pdrop)\n",
    "\n",
    "struct Dense; w; b; f; p; end\n",
    "(d::Dense)(x) = d.f.(d.w * mat(dropout(x,d.p)) .+ d.b) # mat reshapes 4-D tensor to 2-D matrix so we can use matmul\n",
    "Dense(i::Int,o::Int,f=relu;pdrop=0) = Dense(param(o,i), param0(o), f, pdrop)\n",
    "\n",
    "# Perform convolution then, global-max pooling and concatenate the output and feed it to sequential dense layer \n",
    "mutable struct DisModel\n",
    "    charset::Charset\n",
    "    embed::Embed\n",
    "    filters\n",
    "    dense_layers\n",
    "end\n",
    "\n",
    "# This discriminator uses separate weights for its embedding layer\n",
    "function DisModel(charset, embeddingSize::Int, filters, denselayers)\n",
    "    Em = Embed(embed_size, length(tr_charset.c2i))\n",
    "    DisModel(charset, Em, filters, denselayers)\n",
    "end\n",
    "\n",
    "# This discriminator shares the projection layers weights of the generator for its embedding layer\n",
    "function DisModel(charset, embeddingLayer::Embed, filters, denselayers)\n",
    "    DisModel(charset, embeddingLayer, filters, denselayers)\n",
    "end\n",
    "\n",
    "function (c::DisModel)(x) # the input here is weights of the characters with shape (C, B, T)\n",
    "    em = c.embed(x)\n",
    "    filters_out = []\n",
    "    for f in c.filters\n",
    "        push!(filters_out, f(em))\n",
    "    end\n",
    "    out = cat(filters_out...;dims=3)\n",
    "    for l in c.dense_layers\n",
    "        out = l(out)\n",
    "    end\n",
    "    out\n",
    "end\n",
    "\n",
    "(c::DisModel)(x,y; average=true) = nll(c(x), y; average=average)\n",
    "\n",
    "# per-word loss (in this case per-batch loss)\n",
    "function loss(model, data; average=true)\n",
    "    l = 0\n",
    "    n = 0\n",
    "    a = 0\n",
    "    for (x, y) in data\n",
    "        v = model(x, y; average=false)\n",
    "        l += v[1]\n",
    "        n += v[2]\n",
    "        a += (v[1] / v[2])\n",
    "    end\n",
    "    average && return a\n",
    "    return l, n\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Any,1}:\n",
       " \"jeeeebuuffffffffffffffffffffff\"\n",
       " \"LzzzzzzzPPPPPPPPPPPÇÇfffffffff\"\n",
       " \"nyyÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖAAAAzzzzzz\"\n",
       " \"FIIIIISSSSSSSSSSSSzzzzzzzzzfff\"\n",
       " \"ÜIPPPPhyddddzzzzzzzzzzzzzzzzzz\"\n",
       " \"TTTTTTTTTTTÇÇÇÇÇÇÇÇÇÇÇffffffff\"\n",
       " \"jjjjjjjjvvvvvvvvgOOOOOOOOOOOOO\"\n",
       " \"RRRzzzzzzzzzzzzzzzzzzzzzzzzzzz\"\n",
       " \"ĞZZoŞŞŞŞŞŞŞğğğğÇÇÇÇÇÇÇÇÇÇÇÇÇff\"\n",
       " \"UHUUUUUUUUUUUUÜÜÜÜÜÜÜÜÜnnnnnnf\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test generator\n",
    "generate(gmodel, 30, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59×10 Array{Float64,2}:\n",
       " 0.00167831  0.00167831  0.00167831  …  0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00276706     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00276706     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00276706     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831  …  0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831  …  0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " ⋮                                   ⋱                                    \n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831  …  0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831  …  0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831\n",
       " 0.00167831  0.00167831  0.00167831     0.00167831  0.00167831  0.00167831"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau = 2 # normalization factor\n",
    "pi = 1.0 / 59\n",
    "onehot = zeros(59, 10)\n",
    "onehot[[2,3,4],[3,4,5]] .= 1\n",
    "onehot = (onehot .+ log2.(pi)) ./ tau \n",
    "softmax(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 0.0  0.0\n",
       " 0.0  0.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s =zeros(10, 10)\n",
    "s[[2,3], [2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "softmax(x; dims=1, algo=1)\n",
       "\\end{verbatim}\n",
       "The softmax function typically used in classification. Gives the same results as to \\texttt{exp.(logp(x, dims))}. \n",
       "\n",
       "If \\texttt{algo=1} computation is more accurate, if \\texttt{algo=0} it is  faster. \n",
       "\n",
       "See also \\texttt{logsoftmax}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "softmax(x; dims=1, algo=1)\n",
       "```\n",
       "\n",
       "The softmax function typically used in classification. Gives the same results as to `exp.(logp(x, dims))`. \n",
       "\n",
       "If `algo=1` computation is more accurate, if `algo=0` it is  faster. \n",
       "\n",
       "See also `logsoftmax`.\n"
      ],
      "text/plain": [
       "\u001b[36m  softmax(x; dims=1, algo=1)\u001b[39m\n",
       "\n",
       "  The softmax function typically used in classification. Gives the same\n",
       "  results as to \u001b[36mexp.(logp(x, dims))\u001b[39m. \n",
       "\n",
       "  If \u001b[36malgo=1\u001b[39m computation is more accurate, if \u001b[36malgo=0\u001b[39m it is faster. \n",
       "\n",
       "  See also \u001b[36mlogsoftmax\u001b[39m."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
