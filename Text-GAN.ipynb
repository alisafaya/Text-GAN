{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-GAN Turkish word generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mask (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet, Test, Base.Iterators, Printf, LinearAlgebra, CuArrays, Random, IterTools, StatsBase\n",
    "\n",
    "struct Charset\n",
    "    c2i::Dict{Any,Int}\n",
    "    i2c::Vector{Any}\n",
    "    eow::Int\n",
    "end\n",
    "\n",
    "function Charset(charset::String; eow=\"\")\n",
    "    i2c = [ eow; [ c for c in charset ]  ]\n",
    "    print(i2c)\n",
    "    c2i = Dict( c => i for (i, c) in enumerate(i2c))\n",
    "    return Charset(c2i, i2c, c2i[eow])\n",
    "end\n",
    "\n",
    "struct TextReader\n",
    "    file::String\n",
    "    charset::Charset\n",
    "end\n",
    "\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    s === nothing && (s = open(r.file))\n",
    "    eof(s) && return close(s)\n",
    "    return [ get(r.charset.c2i, c, r.charset.eow) for c in readline(s)], s\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}\n",
    "\n",
    "struct WordsData\n",
    "    src::TextReader        \n",
    "    batchsize::Int         \n",
    "    maxlength::Int         \n",
    "    batchmajor::Bool       \n",
    "    bucketwidth::Int    \n",
    "    buckets::Vector        \n",
    "end\n",
    "\n",
    "function WordsData(src::TextReader; batchsize = 128, maxlength = typemax(Int),\n",
    "                batchmajor = false, bucketwidth = 2, numbuckets = min(128, maxlength ÷ bucketwidth))\n",
    "    buckets = [ [] for i in 1:numbuckets ] # buckets[i] is an array of sentence pairs with similar length\n",
    "    WordsData(src, batchsize, maxlength, batchmajor, bucketwidth, buckets)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{WordsData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{WordsData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{WordsData}) = Array{Any,1}\n",
    "\n",
    "function Base.iterate(d::WordsData, state=nothing)\n",
    "    if state == 0 # When file is finished but buckets are partially full \n",
    "        for i in 1:length(d.buckets)\n",
    "            if length(d.buckets[i]) > 0\n",
    "                buc = d.buckets[i]\n",
    "                d.buckets[i] = []\n",
    "                return buc, state\n",
    "            end\n",
    "        end\n",
    "        return nothing # Finish iteration\n",
    "    end\n",
    "\n",
    "    while true\n",
    "        src_next = iterate(d.src, state)\n",
    "        \n",
    "        if src_next === nothing\n",
    "            state = 0\n",
    "            return iterate(d, state)\n",
    "        end\n",
    "        \n",
    "        (src_word, src_state) = src_next\n",
    "        state = src_state\n",
    "        src_length = length(src_word)\n",
    "        \n",
    "        (src_length > d.maxlength) && continue\n",
    "\n",
    "        i = Int(ceil(src_length / d.bucketwidth))\n",
    "        i > length(d.buckets) && (i = length(d.buckets))\n",
    "\n",
    "        push!(d.buckets[i], src_word)\n",
    "        if length(d.buckets[i]) == d.batchsize\n",
    "            buc = d.buckets[i]\n",
    "            d.buckets[i] = []\n",
    "            return buc, state\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function arraybatch(d::WordsData, bucket)\n",
    "    src_eow = d.src.charset.eow\n",
    "    src_lengths = map(x -> length(x), bucket)\n",
    "    max_length = max(src_lengths...)\n",
    "    x = zeros(Int64, length(bucket), d.maxlength + 1) # default d.batchmajor is false\n",
    "\n",
    "    for (i, v) in enumerate(bucket)\n",
    "        to_be_added = fill(src_eow, d.maxlength - length(v))\n",
    "        x[i,:] = [src_eow; v; to_be_added]\n",
    "    end\n",
    "    \n",
    "    d.batchmajor && (x = x')\n",
    "    return (x[:, 1:end-1], x[:, 2:end]) # to calculate nll on generators output directly\n",
    "end\n",
    "\n",
    "function readwordset(fname)\n",
    "    words = []\n",
    "    fi = open(fname)\n",
    "    while !eof(fi)\n",
    "        push!(words, readline(fi))\n",
    "    end\n",
    "    close(fi)\n",
    "    words\n",
    "end\n",
    "\n",
    "function mask(a, pad)\n",
    "    a = copy(a)\n",
    "    for i in 1:size(a, 1)\n",
    "        j = size(a,2)\n",
    "        while a[i, j] == pad && j > 1\n",
    "            if a[i, j - 1] == pad\n",
    "                a[i, j] = 0\n",
    "            end\n",
    "            j -= 1\n",
    "        end\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G/D Common Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Embed; w; end\n",
    "\n",
    "function Embed(shape...)\n",
    "    Embed(param(shape...))\n",
    "end\n",
    "\n",
    "get_z(shape...) = KnetArray(randn(Float32, shape...))\n",
    "\n",
    "# this function is similar to gumble softmax, it is used to soften the one-hot-vector of the real samples\n",
    "# tau -> normalization factor; the bigger the softer\n",
    "function soften(A; dims=1, tau=0.5, norm_factor=0.01) \n",
    "    A = (A .+ norm_factor) ./ tau\n",
    "    softmax(A; dims=dims)\n",
    "end\n",
    "\n",
    "# per-word loss (in this case per-batch loss)\n",
    "function loss(model, data; average=true)\n",
    "    l = 0\n",
    "    n = 0\n",
    "    a = 0\n",
    "    for (x, y) in data\n",
    "        v = model(x, y; average=false)\n",
    "        l += v[1]\n",
    "        n += v[2]\n",
    "        a += (v[1] / v[2])\n",
    "    end\n",
    "    average && return a\n",
    "    return l, n\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one to be used by DModel, takes weights of characters and reduce the embedding for each character\n",
    "# this approach to avoid sampling or argmaxing over rnn's output\n",
    "# (C, B, T) -> (T, E, 1, B)\n",
    "\n",
    "# function (l::Embed)(x)\n",
    "#     dims = size(x)\n",
    "#     em = l.w * reshape(x, dims[1], dims[2] * dims[3]) # reshape for multiplication \n",
    "#     em = reshape(em, size(em, 1), dims[2], dims[3]) # reshape to original size\n",
    "#     em = permutedims(em, [3, 1, 2])  # permute for CONV\n",
    "#     em = reshape(em, dims[3], size(em, 2), 1, dims[2]) # Add one dim for CONV\n",
    "# end\n",
    "\n",
    "# struct Conv; w; b; f; p; end\n",
    "# (c::Conv)(x) = (co=conv4(c.w, dropout(x,c.p)); c.f.(pool((co .+ c.b); window=(size(co, 1), size(co, 2)))))\n",
    "# Conv(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), f, pdrop)\n",
    "\n",
    "# struct Dense; w; b; f; p; end\n",
    "# (d::Dense)(x) = d.f.(d.w * mat(dropout(x,d.p)) .+ d.b) # mat reshapes 4-D tensor to 2-D matrix so we can use matmul\n",
    "# Dense(i::Int,o::Int,f=relu;pdrop=0) = Dense(param(o,i), param0(o), f, pdrop)\n",
    "\n",
    "# # Perform convolution then, global-max pooling and concatenate the output and feed it to sequential dense layer \n",
    "# mutable struct DisModel\n",
    "#     charset::Charset\n",
    "#     embed::Embed\n",
    "#     filters\n",
    "#     dense_layers\n",
    "# end\n",
    "\n",
    "# # This discriminator uses separate weights for its embedding layer\n",
    "# function DisModel(charset, embeddingSize::Int, filters, denselayers)\n",
    "#     Em = Embed(embeddingSize, length(charset.c2i))\n",
    "#     DisModel(charset, Em, filters, denselayers)\n",
    "# end\n",
    "\n",
    "# function (c::DisModel)(x) # the input here is weights of the characters with shape (C, B, T)\n",
    "#     em = c.embed(x)\n",
    "#     filters_out = []\n",
    "#     for f in c.filters\n",
    "#         push!(filters_out, f(em))\n",
    "#     end\n",
    "#     max_out = cat(filters_out...;dims=3)\n",
    "#     for l in c.dense_layers\n",
    "#         max_out = l(max_out)\n",
    "#     end\n",
    "#     max_out\n",
    "# end\n",
    "\n",
    "# (c::DisModel)(x,y; average=true) = nll(c(x), y; average=average)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (l::Embed)(x)\n",
    "    dims = size(x)\n",
    "    em = l.w * reshape(x, dims[1], dims[2] * dims[3]) # reshape for multiplication \n",
    "    em = reshape(em, size(em, 1), dims[2], dims[3]) # reshape to original size\n",
    "end\n",
    "\n",
    "struct Dense; w; b; f; p; end\n",
    "(d::Dense)(x) = d.f.(d.w * mat(dropout(x,d.p)) .+ d.b) # mat reshapes 3-D tensor to 2-D matrix so we can use matmul\n",
    "Dense(i::Int,o::Int,f=relu;pdrop=0) = Dense(param(o,i), param0(o), f, pdrop)\n",
    "\n",
    "mutable struct DisModel\n",
    "    charset::Charset\n",
    "    embed::Embed\n",
    "    rnn::RNN\n",
    "    denselayers\n",
    "end\n",
    "\n",
    "# This discriminator uses separate weights for its embedding layer\n",
    "function DisModel(charset, embeddingSize::Int, hidden, denselayers; layers=1, dropout=0)\n",
    "    Em = Embed(embeddingSize, length(charset.c2i))\n",
    "    rnn = RNN(embeddingSize, hidden; numLayers=layers, dropout=dropout)\n",
    "    DisModel(charset, Em, rnn, denselayers)\n",
    "end\n",
    "\n",
    "function (c::DisModel)(x) # the input here is weights of the characters with shape (C, B, T)\n",
    "    c.rnn.h, c.rnn.c = 0, 0\n",
    "    em = c.embed(x)\n",
    "    rnn_out = c.rnn(em)\n",
    "    dims = size(rnn_out)\n",
    "    rnn_out = reshape(rnn_out, :, dims[2] * dims[3] )\n",
    "    for l in c.denselayers\n",
    "        rnn_out = l(rnn_out)\n",
    "    end\n",
    "    reshape(rnn_out, :, dims[2], dims[3])\n",
    "end\n",
    "\n",
    "function (c::DisModel)(x, reward::Int; average=true)\n",
    "    scores = softmax(c(x))\n",
    "    scores = reshape(scores, :, size(scores, 2) * size(scores, 3))\n",
    "    -log.(scores[1, :])\n",
    "end\n",
    "\n",
    "function (c::DisModel)(x, y; average=true)\n",
    "    scores = reshape(c(x), :, size(y, 1) * size(y, 2))\n",
    "    labels = reshape(y, size(y, 1) * size(y, 2))\n",
    "    return nll(scores, y; average=average)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate (generic function with 1 method)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate z with embedding vectors, z -> (z_size, B), returns (E+z_size, B, T)\n",
    "# this will be used to feed Z to generator at each timestep\n",
    "function (l::Embed)(x, z)\n",
    "    em = l.w[:, x]\n",
    "    z_array = cat((z for i in 1:size(em, 3))...; dims=(3))\n",
    "    cat(em, z_array; dims=(1))\n",
    "end\n",
    "\n",
    "# Generator model\n",
    "struct GenModel\n",
    "    embed::Embed\n",
    "    rnn::RNN        \n",
    "    dropout::Real\n",
    "    charset::Charset\n",
    "    projection::Embed\n",
    "    disModel::DisModel\n",
    "    maxlength::Int\n",
    "    zsize::Int\n",
    "end\n",
    "\n",
    "function GenModel(esize::Int, zsize::Int, hidden::Int, charset::Charset, disModel::DisModel, maxlength::Int; layers=2, dropout=0)\n",
    "    embed = Embed(esize, length(charset.i2c))\n",
    "    rnn = RNN(zsize + esize, hidden; numLayers=layers, dropout=dropout)\n",
    "    projection = Embed(hidden, length(charset.i2c))\n",
    "    GenModel(embed, rnn, dropout, charset, projection, disModel, maxlength, zsize)\n",
    "end\n",
    "\n",
    "# This generator shares the projection layers weights of the discriminator for its projection layer\n",
    "function GenModel(esize::Int, zsize::Int, charset::Charset, disModel::DisModel, maxlength::Int; layers=2, dropout=0)\n",
    "    embed = Embed(esize, length(charset.i2c))\n",
    "    rnn = RNN(zsize + esize, size(disModel.embed.w, 1); numLayers=layers, dropout=dropout)\n",
    "    GenModel(embed, rnn, dropout, charset, disModel.embed, disModel, maxlength, zsize)\n",
    "end\n",
    "\n",
    "# function Z(s::GenModel, batchsize, timesteps)\n",
    "#     z = get_z(s.zsize, batchsize, 1) # according to get_z(H, B, layers)\n",
    "#     return cat([ z for i in 1:timesteps]...;dims=3)\n",
    "# end\n",
    "\n",
    "\n",
    "# # Generator forward pass using only Z as input, size(Z) -> inputsize, batchsize, sequencelength\n",
    "# function (s::GenModel)(Z)\n",
    "#     s.rnn.h, s.rnn.c = 0, 0\n",
    "#     rnn_out = s.rnn(Z) \n",
    "#     dims = size(rnn_out)\n",
    "#     output = s.projection.w' * dropout(reshape(rnn_out, dims[1], dims[2] * dims[3]), s.dropout)\n",
    "#     reshape(softmax(output), size(output, 1), dims[2], dims[3])\n",
    "# end\n",
    "\n",
    "# # Generator forward pass using only Z with argmax and Inputfeeding, size(Z) -> zsize, batchsize, 1\n",
    "# function (s::GenModel)(Z)\n",
    "#     s.rnn.h, s.rnn.c = 0, 0\n",
    "#     input = s.projection(fill(s.charset.eow, size(Z, 2), 1), Z)\n",
    "\n",
    "#     scores = []\n",
    "#     for i in 1:s.maxlength\n",
    "#         rnn_out = s.rnn(input)\n",
    "#         dims = size(rnn_out)\n",
    "#         output = s.projection.w' * reshape(rnn_out, dims[1], dims[2] * dims[3])\n",
    "#         push!(scores, reshape(output, size(output, 1), dims[2], dims[3]))\n",
    "#         input = vcat(s.projection(softmax(scores[end])), Z)\n",
    "#     end\n",
    "\n",
    "#     scores = cat(scores...;dims=3)\n",
    "# end\n",
    "\n",
    "# Generator forward pass using Z and Teacher forcing for input\n",
    "function (s::GenModel)(GenInput) # tuple (input, Z)\n",
    "    (input, _), Z = GenInput\n",
    "    s.rnn.h, s.rnn.c = 0, 0\n",
    "    input = s.embed(input, Z)\n",
    "    rnn_out = s.rnn(input)\n",
    "    dims = size(rnn_out)\n",
    "    output = s.projection.w' * reshape(rnn_out, dims[1], dims[2] * dims[3])\n",
    "    scores = reshape(output, size(output, 1), dims[2], dims[3])\n",
    "end\n",
    "\n",
    "# Generator loss\n",
    "function (s::GenModel)(GenInput, calculateloss::Int; average=true)\n",
    "    # since the discriminator will output 2 for the fake data, \n",
    "    #    we train the generator to get 1 as output from the discriminator\n",
    "    (_, output), Z = GenInput\n",
    "    x = s(GenInput)\n",
    "    dloss = s.disModel(softmax(x), 1)\n",
    "#     average && return mean(dloss)\n",
    "#     return sum(dloss), length(dloss)\n",
    "    \n",
    "#     nll(x, mask(output, s.charset.eow); dims=1, average=average)\n",
    "    \n",
    "    scores = reshape(x, :, size(output, 1) * size(output, 2))\n",
    "    output = mask(reshape(output, size(output, 1) * size(output, 2)), s.charset.eow)\n",
    "    glosses = [nll(scores[:, i], output[i:i]) * dloss[i] for i in 1:size(output, 1) ]\n",
    "    average && return mean(glosses)\n",
    "    return sum(glosses), length(glosses)\n",
    "end\n",
    "\n",
    "function generate(s::GenModel; start=\"\", maxlength=30)\n",
    "    s.rnn.h, s.rnn.c = 0, 0\n",
    "    Z = get_z(s.zsize, 1, 1)\n",
    "    chars = fill(s.charset.eow, 1)\n",
    "\n",
    "    starting_index = 1\n",
    "    for i in 1:length(start)\n",
    "        push!(chars, s.charset.c2i[start[i]])\n",
    "        charembed = s.embed(chars[i:i], Z)\n",
    "        rnn_out = s.rnn(charembed)\n",
    "        starting_index += 1\n",
    "    end\n",
    "    \n",
    "    for i in starting_index:maxlength\n",
    "        charembed = s.embed(chars[i:i], Z)\n",
    "        rnn_out = s.rnn(charembed)\n",
    "        dims = size(rnn_out)\n",
    "        output = s.projection.w' * reshape(rnn_out, dims[1], dims[2] * dims[3])\n",
    "#         push!(chars, s.charset.c2i[ sample(s.charset.i2c, Weights(Array(softmax(reshape(output, length(s.charset.i2c)))))) ] )\n",
    "        push!(chars, argmax(output)[1])\n",
    "        if chars[end] == s.charset.eow\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    join([ s.charset.i2c[i] for i in chars ], \"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word sampler will be used to train discriminator.\n",
    "this sampler should take B, T (batchsize, timestep) as parameters\n",
    "returns (X, Y) tuple \n",
    "where X is tensor of size (C, B, T)\n",
    "and Y is array of size B\n",
    "B consists of real words and generated words\n",
    "C charset size where each value is weight of this char\n",
    "in the case of generated words the generator already gives C, B, T\n",
    "for real words we need to convert words to C, T arrays\n",
    "where every character can be represented by one hot vector or by Gumble-Max (which is normalized one hot vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Sampler\n",
    "    wordsdata::WordsData\n",
    "    charset::Charset\n",
    "    genModel::GenModel\n",
    "    maxBatchsize::Int\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{Sampler}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{Sampler}) = Base.HasEltype()\n",
    "Base.eltype(::Type{Sampler}) = Tuple{KnetArray{Float32,3},Array{Int64,2}}\n",
    "\n",
    "function Base.iterate(s::Sampler, state=nothing)\n",
    "    wdatastate = iterate(s.wordsdata, state)\n",
    "    wdatastate === nothing && (return nothing)\n",
    "    \n",
    "    (bucket, state) = wdatastate\n",
    "    bsize = length(bucket)\n",
    "    src_eow = s.charset.eow\n",
    "    src_lengths = map(x -> length(x), bucket)\n",
    "    max_length = max(src_lengths...)\n",
    "#     gsize = 1 + rand(bsize:s.maxBatchsize) - bsize # count of words to be generated\n",
    "    gsize = bsize\n",
    "    generated = softmax(s.genModel((arraybatch(s.wordsdata, bucket), get_z(s.genModel.zsize, gsize, 1))))\n",
    "\n",
    "    to_be_cat = [generated, ]\n",
    "    for (i, v) in enumerate(bucket)\n",
    "        tindex = [i for i in 1:length(v)]\n",
    "        pindex = [i for i in length(v)+1:s.wordsdata.maxlength]\n",
    "        onehot = KnetArray(zeros(Float32, length(s.charset.c2i), 1, s.wordsdata.maxlength))\n",
    "        onehot[v, :, tindex] .= 1\n",
    "        onehot[s.charset.eow, :, pindex] .= 1\n",
    "        onehot = soften(onehot) # soften one hot vectors elements value\n",
    "        push!(to_be_cat, onehot)\n",
    "    end\n",
    "    x = cat(to_be_cat...;dims=2) # concatenate both generated and sampled words\n",
    "\n",
    "    y = Array(ones(Int, gsize+bsize, s.wordsdata.maxlength)) # create labels 1 -> real, 2-> not-real\n",
    "    y[1:gsize, :] = y[1:gsize, :] .+ 1\n",
    "    \n",
    "    ind = shuffle(1:gsize+bsize) # used to shuffle the batch\n",
    "    x, y = x[:, ind, :], y[ind, :]\n",
    "    return (x,y), state\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train!(model, parameters, trn, dev, tst; lr=0.001)\n",
    "    bestmodel, bestloss = deepcopy(model), loss(model, dev)\n",
    "    progress!(adam(model, trn; lr=lr, params=parameters), seconds=30) do y\n",
    "        devloss = loss(model, dev)\n",
    "        tstloss = loss(model, tst)\n",
    "        if devloss < bestloss\n",
    "            bestloss, bestmodel = devloss, deepcopy(model)\n",
    "        end\n",
    "        println(stderr)\n",
    "        (dev=devloss, tst=tstloss, mem=Float32(CuArrays.usage[]))\n",
    "    end\n",
    "    return bestmodel\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y', 'z', 'Ç', 'Ö', 'Ü', 'ç', 'ö', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş']"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WordsData(TextReader(\"turkish_word_set/dev.tr\", Charset(Dict{Any,Int64}('ç' => 51,'Ğ' => 54,'E' => 6,'Z' => 24,'o' => 39,'B' => 3,'h' => 32,'i' => 33,'r' => 41,'ğ' => 55…), Any[\"\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'  …  'Ü', 'ç', 'ö', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş'], 1)), 128, 15, false, 1, Array{Any,1}[[], [], [], [], [], [], [], [], [], [], [], [], [], [], []])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_set = \"ABCDEFGHIJKLMNOPRSTUVYZabcdefghijklmnoprstuvyzÇÖÜçöüĞğİıŞş\"\n",
    "tr_charset = Charset(char_set)\n",
    "datadir = \"turkish_word_set\"\n",
    "BATCHSIZE = 128\n",
    "MAXLENGTH = 15\n",
    "tr_dev = TextReader(\"$datadir/dev.tr\", tr_charset)\n",
    "tr_trn = TextReader(\"$datadir/train.tr\", tr_charset)\n",
    "dtrn = WordsData(tr_trn, batchsize=BATCHSIZE, maxlength=MAXLENGTH, bucketwidth = 1)\n",
    "ddev = WordsData(tr_dev, batchsize=BATCHSIZE, maxlength=MAXLENGTH, bucketwidth = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sampler(WordsData(TextReader(\"turkish_word_set/dev.tr\", Charset(Dict{Any,Int64}('ç' => 51,'Ğ' => 54,'E' => 6,'Z' => 24,'o' => 39,'B' => 3,'h' => 32,'i' => 33,'r' => 41,'ğ' => 55…), Any[\"\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'  …  'Ü', 'ç', 'ö', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş'], 1)), 128, 15, false, 1, Array{Any,1}[[], [], [], [], [], [], [], [], [], [], [], [], [], [], []]), Charset(Dict{Any,Int64}('ç' => 51,'Ğ' => 54,'E' => 6,'Z' => 24,'o' => 39,'B' => 3,'h' => 32,'i' => 33,'r' => 41,'ğ' => 55…), Any[\"\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'  …  'Ü', 'ç', 'ö', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş'], 1), GenModel(Embed(P(KnetArray{Float32,2}(256,59))), LSTM(input=384,hidden=256,layers=2,dropout=0.1), 0.1, Charset(Dict{Any,Int64}('ç' => 51,'Ğ' => 54,'E' => 6,'Z' => 24,'o' => 39,'B' => 3,'h' => 32,'i' => 33,'r' => 41,'ğ' => 55…), Any[\"\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'  …  'Ü', 'ç', 'ö', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş'], 1), Embed(P(KnetArray{Float32,2}(256,59))), DisModel(Charset(Dict{Any,Int64}('ç' => 51,'Ğ' => 54,'E' => 6,'Z' => 24,'o' => 39,'B' => 3,'h' => 32,'i' => 33,'r' => 41,'ğ' => 55…), Any[\"\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'  …  'Ü', 'ç', 'ö', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş'], 1), Embed(P(KnetArray{Float32,2}(256,59))), LSTM(input=256,hidden=128,dropout=0.3), (Dense(P(KnetArray{Float32,2}(2,128)), P(KnetArray{Float32,1}(2)), identity, 0),)), 15, 128), 256)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 256\n",
    "DHIDDEN_SIZE = 128\n",
    "GDROPOUT = 0.1\n",
    "DDROPOUT = 0.3\n",
    "\n",
    "dismodel = DisModel(tr_charset, EMBEDDING_SIZE, DHIDDEN_SIZE,(\n",
    "        Dense(DHIDDEN_SIZE, 2, identity),\n",
    "        ); dropout=DDROPOUT)\n",
    "\n",
    "GH_SIZE = 256\n",
    "Z_SIZE = 128\n",
    "\n",
    "genmodel = GenModel(EMBEDDING_SIZE, Z_SIZE, GH_SIZE, tr_charset, dismodel, MAXLENGTH; dropout=GDROPOUT, layers=2)\n",
    "trnsampler = Sampler(dtrn, tr_charset, genmodel, BATCHSIZE * 2)\n",
    "devsampler = Sampler(ddev, tr_charset, genmodel, BATCHSIZE * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrn = collect(dtrn)\n",
    "cdev = collect(ddev)\n",
    "collecttrn = [ ((arraybatch(dtrn, i), get_z(Z_SIZE, size(i, 1), 1)), 1) for i in ctrn ]\n",
    "collectdev = [ ((arraybatch(ddev, i), get_z(Z_SIZE, size(i, 1), 1)), 1) for i in cdev ]\n",
    "\n",
    "function gmodel(batches)\n",
    "    global genmodel\n",
    "    global collecttrn\n",
    "    global collectdev\n",
    "    \n",
    "    trnxbatches = shuffle!(collecttrn)[1:batches]\n",
    "    devbatches = shuffle!(collectdev)\n",
    "    trnmini = trnxbatches[1:1]\n",
    "\n",
    "    genmodel = train!(genmodel, params(genmodel)[1:3], trnxbatches, devbatches, trnmini)\n",
    "end\n",
    "\n",
    "function dmodel(batches)\n",
    "    global trnsampler\n",
    "    global devsampler\n",
    "    global dismodel\n",
    "    \n",
    "    ctrn = collect(trnsampler)\n",
    "    ctrn = shuffle!(ctrn)[1:batches]\n",
    "    trnmini = ctrn[1:1]\n",
    "    dev = collect(devsampler)\n",
    "    dismodel = train!(dismodel, params(dismodel), ctrn, dev, trnmini) \n",
    "end\n",
    "\n",
    "@info \"Started training...\"\n",
    "for k in 1:5\n",
    "    println(\"Turn no:\", k)\n",
    "    println(\"Ex.Generated words: \\n\", join([ generate(genmodel; maxlength=MAXLENGTH) for i in 1:5 ],\"\\n\"))\n",
    "    dmodel(50)\n",
    "    gmodel(775)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex.Generated words: \n",
      "hoslanabirleri\n",
      "izilemeklersin\n",
      "kalan\n",
      "MÜNEN\n",
      "SURT\n",
      "Tehtvobimase\n",
      "TenO\n",
      "sayartılardı\n",
      "umaklanıyor\n",
      "içşilemesi\n",
      "Ex.Generated words: \n",
      "En\n",
      "değrek\n",
      "sütüyor\n",
      "yövimin\n",
      "misbafbek\n",
      "guvmen\n",
      "unulüdaması\n",
      "tefpeni\n",
      "cuğumuz\n",
      "KDF\n",
      "Ex.Generated words: \n",
      "KARKERİSİ\n",
      "orda\n",
      "suykgildir\n",
      "basılm\n",
      "eçmenlirin\n",
      "iltinin\n",
      "kapılacağızı\n",
      "curduğunun\n",
      "adetim\n",
      "Tasakl\n",
      "Ex.Generated words: \n",
      "beğtilimler\n",
      "SOTGİ\n",
      "iyduğum\n",
      "kubrasyecedik\n",
      "ırışmık\n",
      "futmandık\n",
      "pitisindarmlır\n",
      "mavistiye\n",
      "alagandan\n",
      "maklama\n",
      "Ex.Generated words: \n",
      "GBavoyors\n",
      "ürglekteki\n",
      "BAŞIGORLEN\n",
      "aarjırdı\n",
      "vetilmeştik\n",
      "Gujlaryı\n",
      "sakır\n",
      "RAYILIZTAR\n",
      "Cabel\n",
      "aldınsı\n",
      "Ex.Generated words: \n",
      "BA\n",
      "nedar\n",
      "snonje\n",
      "ÖLCİRETTASİ\n",
      "ŞRÜZİ\n",
      "Türülübe\n",
      "çakahtaz\n",
      "LÜKİ\n",
      "beserek\n",
      "kalştı\n",
      "Ex.Generated words: \n",
      "eşi\n",
      "hulpplaranı\n",
      "yatındalar\n",
      "ametteya\n",
      "dineklerin\n",
      "TAKLECTİR\n",
      "eğömasın\n",
      "hazeron\n",
      "fafici\n",
      "hons\n",
      "Ex.Generated words: \n",
      "puscuruz\n",
      "NAKAMEN\n",
      "sevoj\n",
      "TASK\n",
      "kustruyorcasını\n",
      "osra\n",
      "pUKAV\n",
      "Adlonga\n",
      "Curdokimri\n",
      "Shivaleri\n",
      "Ex.Generated words: \n",
      "Kardarını\n",
      "Taka\n",
      "umayı\n",
      "dokuruyan\n",
      "Kotanla\n",
      "Trerhur\n",
      "Copondu\n",
      "kon\n",
      "ordumlar\n",
      "vaşlyeye\n",
      "Ex.Generated words: \n",
      "ayıymık\n",
      "kunsedebinden\n",
      "aşmasın\n",
      "zerkeyonunun\n",
      "Vavız\n",
      "ceye\n",
      "Rarabek\n",
      "BUCAM\n",
      "Mi\n",
      "upuyası\n"
     ]
    }
   ],
   "source": [
    "println(\"Ex.Generated words: \\n\", join([ generate(genmodel; maxlength=MAXLENGTH, start=\"\") for i in 1:100 ],\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "Knet.save(\"genmodel.jld2\", \"genmodel\", genmodel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
