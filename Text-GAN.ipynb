{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-GAN Turkish word generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readwordset (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet, Test, Base.Iterators, Printf, LinearAlgebra, CuArrays, Random, IterTools\n",
    "\n",
    "struct Charset\n",
    "    c2i::Dict{Any,Int}\n",
    "    i2c::Vector{Any}\n",
    "    eow::Int\n",
    "end\n",
    "\n",
    "function Charset(charset::String; eow=\"\")\n",
    "    i2c = [ eow; [ c for c in charset ]  ]\n",
    "    print(i2c)\n",
    "    c2i = Dict( c => i for (i, c) in enumerate(i2c))\n",
    "    return Charset(c2i, i2c, c2i[eow])\n",
    "end\n",
    "\n",
    "struct TextReader\n",
    "    file::String\n",
    "    charset::Charset\n",
    "end\n",
    "\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    s === nothing && (s = open(r.file))\n",
    "    eof(s) && return close(s)\n",
    "    return [ get(r.charset.c2i, c, r.charset.eow) for c in readline(s)], s\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}\n",
    "\n",
    "struct WordsData\n",
    "    src::TextReader        \n",
    "    batchsize::Int         \n",
    "    maxlength::Int         \n",
    "    batchmajor::Bool       \n",
    "    bucketwidth::Int    \n",
    "    buckets::Vector        \n",
    "    batchmaker::Function   \n",
    "end\n",
    "\n",
    "function WordsData(src::TextReader; batchmaker = arraybatch, batchsize = 128, maxlength = typemax(Int),\n",
    "                batchmajor = false, bucketwidth = 2, numbuckets = min(128, maxlength ÷ bucketwidth))\n",
    "    buckets = [ [] for i in 1:numbuckets ] # buckets[i] is an array of sentence pairs with similar length\n",
    "    WordsData(src, batchsize, maxlength, batchmajor, bucketwidth, buckets, batchmaker)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{WordsData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{WordsData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{WordsData}) = NTuple{2}\n",
    "\n",
    "function Base.iterate(d::WordsData, state=nothing)\n",
    "    if state == 0 # When file is finished but buckets are partially full \n",
    "        for i in 1:length(d.buckets)\n",
    "            if length(d.buckets[i]) > 0\n",
    "                batch = d.batchmaker(d, d.buckets[i])\n",
    "                d.buckets[i] = []\n",
    "                return batch, state\n",
    "            end\n",
    "        end\n",
    "        return nothing # Finish iteration\n",
    "    elseif state === nothing\n",
    "        # Just to make sure\n",
    "        for i in 1:length(d.buckets)\n",
    "            d.buckets[i] = []\n",
    "        end\n",
    "        state = nothing\n",
    "    end\n",
    "\n",
    "    while true\n",
    "        src_next = iterate(d.src, state)\n",
    "        \n",
    "        if src_next === nothing\n",
    "            state = 0\n",
    "            return iterate(d, state)\n",
    "        end\n",
    "        \n",
    "        (src_word, src_state) = src_next\n",
    "        state = src_state\n",
    "        src_length = length(src_word)\n",
    "        \n",
    "        (src_length > d.maxlength) && continue\n",
    "\n",
    "        i = Int(ceil(src_length / d.bucketwidth))\n",
    "        i > length(d.buckets) && (i = length(d.buckets))\n",
    "\n",
    "        push!(d.buckets[i], src_word)\n",
    "        if length(d.buckets[i]) == d.batchsize\n",
    "            batch = d.batchmaker(d, d.buckets[i])\n",
    "            d.buckets[i] = []\n",
    "            return batch, state\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# function arraybatch(d::WordsData, bucket)\n",
    "#     src_eow = d.src.charset.eow\n",
    "#     src_lengths = map(x -> length(x), bucket)\n",
    "#     max_length = max(src_lengths...)\n",
    "#     x = zeros(Int64, length(bucket), max_length + 2) # default d.batchmajor is false\n",
    "\n",
    "#     for (i, v) in enumerate(bucket)\n",
    "#         to_be_added = fill(src_eow, max_length - length(v) + 1)\n",
    "#         x[i,:] = [src_eow; v; to_be_added]\n",
    "#     end\n",
    "\n",
    "#     d.batchmajor && (x = x')\n",
    "#     return (x[:, 1:end-1], x[:, 2:end])\n",
    "# end\n",
    "\n",
    "function readwordset(fname)\n",
    "    words = []\n",
    "    fi = open(fname)\n",
    "    while !eof(fi)\n",
    "        push!(words, readline(fi))\n",
    "    end\n",
    "    close(fi)\n",
    "    words\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G/D/S Common Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embed"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Embed; w; end\n",
    "\n",
    "function Embed(shape...)\n",
    "    Embed(param(shape...))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_z(shape...) = KnetArray(randn(Float32, shape...))\n",
    "\n",
    "\n",
    "### Not used \n",
    "# concatenate z with embedding vectors, z -> (z_size, B), returns (E+z_size, B, T)\n",
    "# this will be used to feed Z to generator at each timestep\n",
    "# function (l::Embed)(x, z)\n",
    "#     em = l.w[:, x]\n",
    "#     z_array = cat((z for i in 1:size(em, 3))...; dims=(3))\n",
    "#     cat(em, z_array; dims=(1))\n",
    "# end\n",
    "\n",
    "# Generator model\n",
    "struct GModel\n",
    "    projection::Embed\n",
    "    rnn::RNN        \n",
    "    dropout::Real\n",
    "    charset::Charset \n",
    "end\n",
    "\n",
    "function GModel(hidden::Int, charset::Charset; layers=2, dropout=0)\n",
    "    rnn = RNN(1, hidden; numLayers=layers, dropout=dropout) # input size is 1\n",
    "    projection = Embed(hidden, length(charset.i2c))\n",
    "    GModel(projection, rnn, dropout, charset)\n",
    "end\n",
    "\n",
    "# Generator forward pass, here Z is our latent var -> (H, Tx, )\n",
    "function (s::GModel)(timesteps, batchsize)\n",
    "    s.rnn.h = get_z(s.rnn.hiddenSize, batchsize, s.rnn.numLayers) # according to get_z(H, B, layers)\n",
    "    s.rnn.c = get_z(s.rnn.hiddenSize, batchsize, s.rnn.numLayers) # according to get_z(H, B, layers)\n",
    "    rnn_out = s.rnn(KnetArray(ones(Float32, (1, batchsize, timesteps))))\n",
    "    dims = size(rnn_out)\n",
    "    output = s.projection.w' * dropout(reshape(rnn_out, dims[1], dims[2] * dims[3]), s.dropout)\n",
    "    reshape(softmax(output), size(output, 1), dims[2], dims[3])\n",
    "end\n",
    "\n",
    "function generate(s::GModel, maxlength, batchsize)\n",
    "    out = s(maxlength, batchsize)\n",
    "    words = []\n",
    "    for i in 1:batchsize\n",
    "        push!(words, join([s.charset.i2c[x[1]] for x in argmax(out[:, i, :]; dims=1)], \"\"))\n",
    "    end\n",
    "    words\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: word sampler will be used to train discriminator. \n",
    "this sampler should take B, T (batchsize, timestep) as parameters\n",
    "returns (X, Y) tuple \n",
    "where X is tensor of size (C, B, T)\n",
    "and Y is array of size B\n",
    "B consists of real words and generated words\n",
    "C charset size where each value is weight of this char\n",
    "in the case of generated words the generator already gives C, B, T\n",
    "for real words we need to convert words to C, T arrays\n",
    "where every character can be represented by one hot vector or by Gumble-Max (which is normalized one hot vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Sampler\n",
    "    \n",
    "end\n",
    "\n",
    "function Base.iterate(s::Sampler, state=nothing)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one to be used by DModel, takes weights of characters and reduce the embedding for each character\n",
    "# this approach to avoid sampling or argmaxing over rnn's output\n",
    "# (C, B, T) -> (T, E, 1, B)\n",
    "function (l::Embed)(x)\n",
    "    dims = size(out)\n",
    "    em = l.w * reshape(x, dims[1], dims[2] * dims[3]) # reshape for multiplication \n",
    "    em = reshape(em, size(em, 1), dims[2], dims[3]) # reshape to original size\n",
    "    em = permutedims(em, [3, 1, 2])  # permute for CONV\n",
    "    em = reshape(em, dims[3], size(em, 2), 1, dims[2]) # Add one dim for CONV\n",
    "end\n",
    "\n",
    "struct Conv; w; b; f; p; end\n",
    "(c::Conv)(x) = (co=conv4(c.w, dropout(x,c.p)); c.f.(pool((co .+ c.b); window=(size(co, 1), size(co, 2)))))\n",
    "Conv(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), f, pdrop)\n",
    "\n",
    "struct Dense; w; b; f; p; end\n",
    "(d::Dense)(x) = d.f.(d.w * mat(dropout(x,d.p)) .+ d.b) # mat reshapes 4-D tensor to 2-D matrix so we can use matmul\n",
    "Dense(i::Int,o::Int,f=relu;pdrop=0) = Dense(param(o,i), param0(o), f, pdrop)\n",
    "\n",
    "# Perform convolution then, global-max pooling and concatenate the output and feed it to sequential dense layer \n",
    "mutable struct DisModel\n",
    "    charset::Charset\n",
    "    embed::Embed\n",
    "    filters\n",
    "    dense_layers\n",
    "end\n",
    "\n",
    "# This discriminator uses separate weights for its embedding layer\n",
    "function DisModel(charset, embeddingSize::Int, filters, denselayers)\n",
    "    Em = Embed(embed_size, length(tr_charset.c2i))\n",
    "    DisModel(charset, Em, filters, denselayers)\n",
    "end\n",
    "\n",
    "# This discriminator shares the projection layers weights of the generator for its embedding layer\n",
    "function DisModel(charset, embeddingLayer::Embed, filters, denselayers)\n",
    "    DisModel(charset, embeddingLayer, filters, denselayers)\n",
    "end\n",
    "\n",
    "function (c::DisModel)(x) # the input here is weights of the characters with shape (C, B, T)\n",
    "    em = c.embed(x)\n",
    "    filters_out = []\n",
    "    for f in c.filters\n",
    "        push!(filters_out, f(em))\n",
    "    end\n",
    "    out = cat(filters_out...;dims=3)\n",
    "    for l in c.dense_layers\n",
    "        out = l(out)\n",
    "    end\n",
    "    out\n",
    "end\n",
    "\n",
    "(c::DisModel)(x,y; average=true) = nll(c(x), y; average=average)\n",
    "\n",
    "# per-word loss (in this case per-batch loss)\n",
    "function loss(model, data; average=true)\n",
    "    l = 0\n",
    "    n = 0\n",
    "    a = 0\n",
    "    for (x, y) in data\n",
    "        v = model(x, y; average=false)\n",
    "        l += v[1]\n",
    "        n += v[2]\n",
    "        a += (v[1] / v[2])\n",
    "    end\n",
    "    average && return a\n",
    "    return l, n\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y', 'z', 'Ç', 'Ö', 'Ü', 'ç', 'ö', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş']"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2×16 KnetArray{Float32,2}:\n",
       " 0.499999  0.499999  0.5       0.499999  …  0.5  0.499999  0.499999  0.5\n",
       " 0.499999  0.499999  0.499999  0.499999     0.5  0.499999  0.5       0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_set = \"ABCDEFGHIJKLMNOPRSTUVYZabcdefghijklmnoprstuvyzÇÖÜçöüĞğİıŞş\"\n",
    "tr_charset = Charset(char_set)\n",
    "\n",
    "embedding_size = 64\n",
    "gmodel = GModel(embedding_size, tr_charset; dropout=0.2)\n",
    "out = gmodel(30, 16) # (T, B) -> (C, B, T)\n",
    "\n",
    "# generate(gmodel, 30, 10)\n",
    "filter_no = 20\n",
    "dismodel = DisModel(tr_charset, gmodel.projection, (\n",
    "        Conv(2,embedding_size,1,filter_no; pdrop=0.2),\n",
    "        Conv(3,embedding_size,1,filter_no; pdrop=0.2),\n",
    "        Conv(4,embedding_size,1,filter_no; pdrop=0.2),\n",
    "        ),(\n",
    "        Dense(60,64,pdrop=0.3),\n",
    "        Dense(64,2,sigm,pdrop=0.3)\n",
    "        ))\n",
    "\n",
    "\n",
    "dismodel(gmodel(30, 16)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
