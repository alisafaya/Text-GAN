{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-GAN Turkish word generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mask (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet, Test, Base.Iterators, Printf, LinearAlgebra, CuArrays, Random, IterTools, StatsBase\n",
    "\n",
    "struct Charset\n",
    "    c2i::Dict{Any,Int}\n",
    "    i2c::Vector{Any}\n",
    "    eow::Int\n",
    "end\n",
    "\n",
    "function Charset(charset::String; eow=\"\")\n",
    "    i2c = [ eow; [ c for c in charset ]  ]\n",
    "    print(i2c)\n",
    "    c2i = Dict( c => i for (i, c) in enumerate(i2c))\n",
    "    return Charset(c2i, i2c, c2i[eow])\n",
    "end\n",
    "\n",
    "struct TextReader\n",
    "    file::String\n",
    "    charset::Charset\n",
    "end\n",
    "\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    s === nothing && (s = open(r.file))\n",
    "    eof(s) && return close(s)\n",
    "    return [ get(r.charset.c2i, c, r.charset.eow) for c in readline(s)], s\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}\n",
    "\n",
    "struct WordsData\n",
    "    src::TextReader        \n",
    "    batchsize::Int         \n",
    "    maxlength::Int         \n",
    "    batchmajor::Bool       \n",
    "    bucketwidth::Int    \n",
    "    buckets::Vector        \n",
    "end\n",
    "\n",
    "function WordsData(src::TextReader; batchsize = 128, maxlength = typemax(Int),\n",
    "                batchmajor = false, bucketwidth = 2, numbuckets = min(128, maxlength ÷ bucketwidth))\n",
    "    buckets = [ [] for i in 1:numbuckets ] # buckets[i] is an array of sentence pairs with similar length\n",
    "    WordsData(src, batchsize, maxlength, batchmajor, bucketwidth, buckets)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{WordsData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{WordsData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{WordsData}) = Array{Any,1}\n",
    "\n",
    "function Base.iterate(d::WordsData, state=nothing)\n",
    "    if state == 0 # When file is finished but buckets are partially full \n",
    "        for i in 1:length(d.buckets)\n",
    "            if length(d.buckets[i]) > 0\n",
    "                buc = d.buckets[i]\n",
    "                d.buckets[i] = []\n",
    "                return buc, state\n",
    "            end\n",
    "        end\n",
    "        return nothing # Finish iteration\n",
    "    end\n",
    "\n",
    "    while true\n",
    "        src_next = iterate(d.src, state)\n",
    "        \n",
    "        if src_next === nothing\n",
    "            state = 0\n",
    "            return iterate(d, state)\n",
    "        end\n",
    "        \n",
    "        (src_word, src_state) = src_next\n",
    "        state = src_state\n",
    "        src_length = length(src_word)\n",
    "        \n",
    "        (src_length > d.maxlength) && continue\n",
    "\n",
    "        i = Int(ceil(src_length / d.bucketwidth))\n",
    "        i > length(d.buckets) && (i = length(d.buckets))\n",
    "\n",
    "        push!(d.buckets[i], src_word)\n",
    "        if length(d.buckets[i]) == d.batchsize\n",
    "            buc = d.buckets[i]\n",
    "            d.buckets[i] = []\n",
    "            return buc, state\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function arraybatch(d::WordsData, bucket)\n",
    "    src_eow = d.src.charset.eow\n",
    "    src_lengths = map(x -> length(x), bucket)\n",
    "    max_length = max(src_lengths...)\n",
    "    x = zeros(Int64, length(bucket), d.maxlength + 1) # default d.batchmajor is false\n",
    "\n",
    "    for (i, v) in enumerate(bucket)\n",
    "        to_be_added = fill(src_eow, d.maxlength - length(v))\n",
    "        x[i,:] = [src_eow; v; to_be_added]\n",
    "    end\n",
    "    \n",
    "    d.batchmajor && (x = x')\n",
    "    return (x[:, 1:end-1], x[:, 2:end]) # to calculate nll on generators output directly\n",
    "end\n",
    "\n",
    "function readwordset(fname)\n",
    "    words = []\n",
    "    fi = open(fname)\n",
    "    while !eof(fi)\n",
    "        push!(words, readline(fi))\n",
    "    end\n",
    "    close(fi)\n",
    "    words\n",
    "end\n",
    "\n",
    "function mask(a, pad)\n",
    "    a = copy(a)\n",
    "    for i in 1:size(a, 1)\n",
    "        j = size(a,2)\n",
    "        while a[i, j] == pad && j > 1\n",
    "            if a[i, j - 1] == pad\n",
    "                a[i, j] = 0\n",
    "            end\n",
    "            j -= 1\n",
    "        end\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G/D Common Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Embed; w; end\n",
    "\n",
    "function Embed(shape...)\n",
    "    Embed(param(shape...))\n",
    "end\n",
    "\n",
    "get_z(shape...) = KnetArray(randn(Float32, shape...))\n",
    "\n",
    "# this function is similar to gumble softmax, it is used to soften the one-hot-vector of the real samples\n",
    "# tau -> normalization factor; the bigger the softer\n",
    "function soften(A; dims=1, tau=0.5, norm_factor=0.01) \n",
    "    A = (A .+ norm_factor) ./ tau\n",
    "    softmax(A; dims=dims)\n",
    "end\n",
    "\n",
    "# per-word loss (in this case per-batch loss)\n",
    "function loss(model, data; average=true)\n",
    "    l = 0\n",
    "    n = 0\n",
    "    a = 0\n",
    "    for (x, y) in data\n",
    "        v = model(x, y; average=false)\n",
    "        l += v[1]\n",
    "        n += v[2]\n",
    "        a += (v[1] / v[2])\n",
    "    end\n",
    "    average && return a\n",
    "    return l, n\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one to be used by DModel, takes weights of characters and reduce the embedding for each character\n",
    "# this approach to avoid sampling or argmaxing over rnn's output\n",
    "# (C, B, T) -> (T, E, 1, B)\n",
    "\n",
    "# function (l::Embed)(x)\n",
    "#     dims = size(x)\n",
    "#     em = l.w * reshape(x, dims[1], dims[2] * dims[3]) # reshape for multiplication \n",
    "#     em = reshape(em, size(em, 1), dims[2], dims[3]) # reshape to original size\n",
    "#     em = permutedims(em, [3, 1, 2])  # permute for CONV\n",
    "#     em = reshape(em, dims[3], size(em, 2), 1, dims[2]) # Add one dim for CONV\n",
    "# end\n",
    "\n",
    "# struct Conv; w; b; f; p; end\n",
    "# (c::Conv)(x) = (co=conv4(c.w, dropout(x,c.p)); c.f.(pool((co .+ c.b); window=(size(co, 1), size(co, 2)))))\n",
    "# Conv(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), f, pdrop)\n",
    "\n",
    "# struct Dense; w; b; f; p; end\n",
    "# (d::Dense)(x) = d.f.(d.w * mat(dropout(x,d.p)) .+ d.b) # mat reshapes 4-D tensor to 2-D matrix so we can use matmul\n",
    "# Dense(i::Int,o::Int,f=relu;pdrop=0) = Dense(param(o,i), param0(o), f, pdrop)\n",
    "\n",
    "# # Perform convolution then, global-max pooling and concatenate the output and feed it to sequential dense layer \n",
    "# mutable struct DisModel\n",
    "#     charset::Charset\n",
    "#     embed::Embed\n",
    "#     filters\n",
    "#     dense_layers\n",
    "# end\n",
    "\n",
    "# # This discriminator uses separate weights for its embedding layer\n",
    "# function DisModel(charset, embeddingSize::Int, filters, denselayers)\n",
    "#     Em = Embed(embeddingSize, length(charset.c2i))\n",
    "#     DisModel(charset, Em, filters, denselayers)\n",
    "# end\n",
    "\n",
    "# function (c::DisModel)(x) # the input here is weights of the characters with shape (C, B, T)\n",
    "#     em = c.embed(x)\n",
    "#     filters_out = []\n",
    "#     for f in c.filters\n",
    "#         push!(filters_out, f(em))\n",
    "#     end\n",
    "#     max_out = cat(filters_out...;dims=3)\n",
    "#     for l in c.dense_layers\n",
    "#         max_out = l(max_out)\n",
    "#     end\n",
    "#     max_out\n",
    "# end\n",
    "\n",
    "# (c::DisModel)(x,y; average=true) = nll(c(x), y; average=average)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (l::Embed)(x)\n",
    "    dims = size(x)\n",
    "    em = l.w * reshape(x, dims[1], dims[2] * dims[3]) # reshape for multiplication \n",
    "    em = reshape(em, size(em, 1), dims[2], dims[3]) # reshape to original size\n",
    "end\n",
    "\n",
    "struct Dense; w; b; f; p; end\n",
    "(d::Dense)(x) = d.f.(d.w * mat(dropout(x,d.p)) .+ d.b) # mat reshapes 3-D tensor to 2-D matrix so we can use matmul\n",
    "Dense(i::Int,o::Int,f=relu;pdrop=0) = Dense(param(o,i), param0(o), f, pdrop)\n",
    "\n",
    "mutable struct DisModel\n",
    "    charset::Charset\n",
    "    embed::Embed\n",
    "    rnn::RNN\n",
    "    denselayers\n",
    "end\n",
    "\n",
    "# This discriminator uses separate weights for its embedding layer\n",
    "function DisModel(charset, embeddingSize::Int, hidden, denselayers; layers=1, dropout=0)\n",
    "    Em = Embed(embeddingSize, length(charset.c2i))\n",
    "    rnn = RNN(embeddingSize, hidden; numLayers=layers, dropout=dropout)\n",
    "    DisModel(charset, Em, rnn, denselayers)\n",
    "end\n",
    "\n",
    "function (c::DisModel)(x) # the input here is weights of the characters with shape (C, B, T)\n",
    "    c.rnn.h, c.rnn.c = 0, 0\n",
    "    em = c.embed(x)\n",
    "    rnn_out = c.rnn(em)\n",
    "    dims = size(rnn_out)\n",
    "    rnn_out = reshape(rnn_out, :, dims[2] * dims[3] )\n",
    "    for l in c.denselayers\n",
    "        rnn_out = l(rnn_out)\n",
    "    end\n",
    "    reshape(rnn_out, :, dims[2], dims[3])\n",
    "end\n",
    "\n",
    "function (c::DisModel)(x, reward::Int; average=true)\n",
    "    scores = softmax(c(x))\n",
    "    scores = reshape(scores, :, size(scores, 2) * size(scores, 3))\n",
    "    -log.(scores[1, :])\n",
    "end\n",
    "\n",
    "function (c::DisModel)(x, y; average=true)\n",
    "    scores = reshape(c(x), :, size(y, 1) * size(y, 2))\n",
    "    labels = reshape(y, size(y, 1) * size(y, 2))\n",
    "    return nll(scores, y; average=average)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate z with embedding vectors, z -> (z_size, B), returns (E+z_size, B, T)\n",
    "# this will be used to feed Z to generator at each timestep\n",
    "function (l::Embed)(x, z)\n",
    "    em = l.w[:, x]\n",
    "    z_array = cat((z for i in 1:size(em, 3))...; dims=(3))\n",
    "    cat(em, z_array; dims=(1))\n",
    "end\n",
    "\n",
    "# Generator model\n",
    "struct GenModel\n",
    "    embed::Embed\n",
    "    rnn::RNN        \n",
    "    dropout::Real\n",
    "    charset::Charset\n",
    "    projection::Embed\n",
    "    disModel::DisModel\n",
    "    maxlength::Int\n",
    "    zsize::Int\n",
    "end\n",
    "\n",
    "function GenModel(esize::Int, zsize::Int, hidden::Int, charset::Charset, disModel::DisModel, maxlength::Int; layers=2, dropout=0)\n",
    "    embed = Embed(esize, length(charset.i2c))\n",
    "    rnn = RNN(zsize + esize, hidden; numLayers=layers, dropout=dropout)\n",
    "    projection = Embed(hidden, length(charset.i2c))\n",
    "    GenModel(embed, rnn, dropout, charset, projection, disModel, maxlength, zsize)\n",
    "end\n",
    "\n",
    "# This generator shares the projection layers weights of the discriminator for its projection layer\n",
    "function GenModel(esize::Int, zsize::Int, charset::Charset, disModel::DisModel, maxlength::Int; layers=2, dropout=0)\n",
    "    embed = Embed(esize, length(charset.i2c))\n",
    "    rnn = RNN(zsize + esize, size(disModel.embed.w, 1); numLayers=layers, dropout=dropout)\n",
    "    GenModel(embed, rnn, dropout, charset, disModel.embed, disModel, maxlength, zsize)\n",
    "end\n",
    "\n",
    "# function Z(s::GenModel, batchsize, timesteps)\n",
    "#     z = get_z(s.zsize, batchsize, 1) # according to get_z(H, B, layers)\n",
    "#     return cat([ z for i in 1:timesteps]...;dims=3)\n",
    "# end\n",
    "\n",
    "\n",
    "# # Generator forward pass using only Z as input, size(Z) -> inputsize, batchsize, sequencelength\n",
    "# function (s::GenModel)(Z)\n",
    "#     s.rnn.h, s.rnn.c = 0, 0\n",
    "#     rnn_out = s.rnn(Z) \n",
    "#     dims = size(rnn_out)\n",
    "#     output = s.projection.w' * dropout(reshape(rnn_out, dims[1], dims[2] * dims[3]), s.dropout)\n",
    "#     reshape(softmax(output), size(output, 1), dims[2], dims[3])\n",
    "# end\n",
    "\n",
    "# # Generator forward pass using only Z with argmax and Inputfeeding, size(Z) -> zsize, batchsize, 1\n",
    "# function (s::GenModel)(Z)\n",
    "#     s.rnn.h, s.rnn.c = 0, 0\n",
    "#     input = s.projection(fill(s.charset.eow, size(Z, 2), 1), Z)\n",
    "\n",
    "#     scores = []\n",
    "#     for i in 1:s.maxlength\n",
    "#         rnn_out = s.rnn(input)\n",
    "#         dims = size(rnn_out)\n",
    "#         output = s.projection.w' * reshape(rnn_out, dims[1], dims[2] * dims[3])\n",
    "#         push!(scores, reshape(output, size(output, 1), dims[2], dims[3]))\n",
    "#         input = vcat(s.projection(softmax(scores[end])), Z)\n",
    "#     end\n",
    "\n",
    "#     scores = cat(scores...;dims=3)\n",
    "# end\n",
    "\n",
    "# Generator forward pass using Z and Teacher forcing for input\n",
    "function (s::GenModel)(GenInput) # tuple (input, Z)\n",
    "    (input, _), Z = GenInput\n",
    "    s.rnn.h, s.rnn.c = 0, 0\n",
    "    input = s.embed(input, Z)\n",
    "    rnn_out = s.rnn(input)\n",
    "    dims = size(rnn_out)\n",
    "    output = s.projection.w' * reshape(rnn_out, dims[1], dims[2] * dims[3])\n",
    "    scores = reshape(output, size(output, 1), dims[2], dims[3])\n",
    "end\n",
    "\n",
    "# Generator loss\n",
    "function (s::GenModel)(GenInput, calculateloss::Int; average=true)\n",
    "    # since the discriminator will output 2 for the fake data, \n",
    "    #    we train the generator to get 1 as output from the discriminator\n",
    "    (_, output), Z = GenInput\n",
    "    x = s(GenInput)\n",
    "    dloss = s.disModel(softmax(x), 1)\n",
    "#     average && return mean(dloss)\n",
    "#     return sum(dloss), length(dloss)\n",
    "    \n",
    "#     nll(x, mask(output, s.charset.eow); dims=1, average=average)\n",
    "    \n",
    "    scores = reshape(x, :, size(output, 1) * size(output, 2))\n",
    "    output = mask(reshape(output, size(output, 1) * size(output, 2)), s.charset.eow)\n",
    "    glosses = [nll(scores[:, i], output[i:i]) * dloss[i] for i in 1:size(output, 1) ]\n",
    "    average && return mean(glosses)\n",
    "    return sum(glosses), length(glosses)\n",
    "end\n",
    "\n",
    "function generate(s::GenModel; start=\"\", maxlength=30)\n",
    "    s.rnn.h, s.rnn.c = 0, 0\n",
    "    Z = get_z(s.zsize, 1, 1)\n",
    "    chars = fill(s.charset.eow, 1)\n",
    "\n",
    "    starting_index = 1\n",
    "    for i in 1:length(start)\n",
    "        push!(chars, s.charset.c2i[start[i]])\n",
    "        charembed = s.embed(chars[i:i], Z)\n",
    "        rnn_out = s.rnn(charembed)\n",
    "        starting_index += 1\n",
    "    end\n",
    "    \n",
    "    for i in starting_index:maxlength\n",
    "        charembed = s.embed(chars[i:i], Z)\n",
    "        rnn_out = s.rnn(charembed)\n",
    "        dims = size(rnn_out)\n",
    "        output = s.projection.w' * reshape(rnn_out, dims[1], dims[2] * dims[3])\n",
    "#         push!(chars, s.charset.c2i[ sample(s.charset.i2c, Weights(Array(softmax(reshape(output, length(s.charset.i2c)))))) ] )\n",
    "        push!(chars, argmax(output)[1])\n",
    "        if chars[end] == s.charset.eow\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    join([ s.charset.i2c[i] for i in chars ], \"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word sampler will be used to train discriminator.\n",
    "this sampler should take B, T (batchsize, timestep) as parameters\n",
    "returns (X, Y) tuple \n",
    "where X is tensor of size (C, B, T)\n",
    "and Y is array of size B\n",
    "B consists of real words and generated words\n",
    "C charset size where each value is weight of this char\n",
    "in the case of generated words the generator already gives C, B, T\n",
    "for real words we need to convert words to C, T arrays\n",
    "where every character can be represented by one hot vector or by Gumble-Max (which is normalized one hot vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Sampler\n",
    "    wordsdata::WordsData\n",
    "    charset::Charset\n",
    "    genModel::GenModel\n",
    "    maxBatchsize::Int\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{Sampler}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{Sampler}) = Base.HasEltype()\n",
    "Base.eltype(::Type{Sampler}) = Tuple{KnetArray{Float32,3},Array{Int64,2}}\n",
    "\n",
    "function Base.iterate(s::Sampler, state=nothing)\n",
    "    wdatastate = iterate(s.wordsdata, state)\n",
    "    wdatastate === nothing && (return nothing)\n",
    "    \n",
    "    (bucket, state) = wdatastate\n",
    "    bsize = length(bucket)\n",
    "    src_eow = s.charset.eow\n",
    "    src_lengths = map(x -> length(x), bucket)\n",
    "    max_length = max(src_lengths...)\n",
    "#     gsize = 1 + rand(bsize:s.maxBatchsize) - bsize # count of words to be generated\n",
    "    gsize = bsize\n",
    "    generated = softmax(s.genModel((arraybatch(s.wordsdata, bucket), get_z(s.genModel.zsize, gsize, 1))))\n",
    "\n",
    "    to_be_cat = [generated, ]\n",
    "    for (i, v) in enumerate(bucket)\n",
    "        tindex = [i for i in 1:length(v)]\n",
    "        pindex = [i for i in length(v)+1:s.wordsdata.maxlength]\n",
    "        onehot = KnetArray(zeros(Float32, length(s.charset.c2i), 1, s.wordsdata.maxlength))\n",
    "        onehot[v, :, tindex] .= 1\n",
    "        onehot[s.charset.eow, :, pindex] .= 1\n",
    "        onehot = soften(onehot) # soften one hot vectors elements value\n",
    "        push!(to_be_cat, onehot)\n",
    "    end\n",
    "    x = cat(to_be_cat...;dims=2) # concatenate both generated and sampled words\n",
    "\n",
    "    y = Array(ones(Int, gsize+bsize, s.wordsdata.maxlength)) # create labels 1 -> real, 2-> not-real\n",
    "    y[1:gsize, :] = y[1:gsize, :] .+ 1\n",
    "    \n",
    "    ind = shuffle(1:gsize+bsize) # used to shuffle the batch\n",
    "    x, y = x[:, ind, :], y[ind, :]\n",
    "    return (x,y), state\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train!(model, parameters, trn, dev, tst; lr=0.001)\n",
    "    bestmodel, bestloss = deepcopy(model), loss(model, dev)\n",
    "    progress!(adam(model, trn; lr=lr, params=parameters), seconds=30) do y\n",
    "        devloss = loss(model, dev)\n",
    "        tstloss = loss(model, tst)\n",
    "        if devloss < bestloss\n",
    "            bestloss, bestmodel = devloss, deepcopy(model)\n",
    "        end\n",
    "        println(stderr)\n",
    "        (dev=devloss, tst=tstloss, mem=Float32(CuArrays.usage[]))\n",
    "    end\n",
    "    return bestmodel\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y', 'z', 'Ç', 'Ö', 'Ü', 'ç', 'ö', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş']"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WordsData(TextReader(\"turkish_word_set/dev.tr\", Charset(Dict{Any,Int64}('ç' => 51,'Ğ' => 54,'E' => 6,'Z' => 24,'o' => 39,'B' => 3,'h' => 32,'i' => 33,'r' => 41,'ğ' => 55…), Any[\"\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'  …  'Ü', 'ç', 'ö', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş'], 1)), 128, 15, false, 1, Array{Any,1}[[], [], [], [], [], [], [], [], [], [], [], [], [], [], []])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_set = \"ABCDEFGHIJKLMNOPRSTUVYZabcdefghijklmnoprstuvyzÇÖÜçöüĞğİıŞş\"\n",
    "tr_charset = Charset(char_set)\n",
    "datadir = \"turkish_word_set\"\n",
    "BATCHSIZE = 128\n",
    "MAXLENGTH = 15\n",
    "tr_dev = TextReader(\"$datadir/dev.tr\", tr_charset)\n",
    "tr_trn = TextReader(\"$datadir/train.tr\", tr_charset)\n",
    "dtrn = WordsData(tr_trn, batchsize=BATCHSIZE, maxlength=MAXLENGTH, bucketwidth = 1)\n",
    "ddev = WordsData(tr_dev, batchsize=BATCHSIZE, maxlength=MAXLENGTH, bucketwidth = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sampler(WordsData(TextReader(\"turkish_word_set/dev.tr\", Charset(Dict{Any,Int64}('ç' => 51,'Ğ' => 54,'E' => 6,'Z' => 24,'o' => 39,'B' => 3,'h' => 32,'i' => 33,'r' => 41,'ğ' => 55…), Any[\"\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'  …  'Ü', 'ç', 'ö', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş'], 1)), 128, 15, false, 1, Array{Any,1}[[], [], [], [], [], [], [], [], [], [], [], [], [], [], []]), Charset(Dict{Any,Int64}('ç' => 51,'Ğ' => 54,'E' => 6,'Z' => 24,'o' => 39,'B' => 3,'h' => 32,'i' => 33,'r' => 41,'ğ' => 55…), Any[\"\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'  …  'Ü', 'ç', 'ö', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş'], 1), GenModel(Embed(P(KnetArray{Float32,2}(256,59))), LSTM(input=384,hidden=256,layers=2,dropout=0.1), 0.1, Charset(Dict{Any,Int64}('ç' => 51,'Ğ' => 54,'E' => 6,'Z' => 24,'o' => 39,'B' => 3,'h' => 32,'i' => 33,'r' => 41,'ğ' => 55…), Any[\"\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'  …  'Ü', 'ç', 'ö', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş'], 1), Embed(P(KnetArray{Float32,2}(256,59))), DisModel(Charset(Dict{Any,Int64}('ç' => 51,'Ğ' => 54,'E' => 6,'Z' => 24,'o' => 39,'B' => 3,'h' => 32,'i' => 33,'r' => 41,'ğ' => 55…), Any[\"\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'  …  'Ü', 'ç', 'ö', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş'], 1), Embed(P(KnetArray{Float32,2}(256,59))), LSTM(input=256,hidden=128,dropout=0.3), (Dense(P(KnetArray{Float32,2}(2,128)), P(KnetArray{Float32,1}(2)), identity, 0),)), 15, 128), 256)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 256\n",
    "DHIDDEN_SIZE = 128\n",
    "GDROPOUT = 0.1\n",
    "DDROPOUT = 0.3\n",
    "\n",
    "dismodel = DisModel(tr_charset, EMBEDDING_SIZE, DHIDDEN_SIZE,(\n",
    "        Dense(DHIDDEN_SIZE, 2, identity),\n",
    "        ); dropout=DDROPOUT)\n",
    "\n",
    "GH_SIZE = 256\n",
    "Z_SIZE = 128\n",
    "\n",
    "genmodel = GenModel(EMBEDDING_SIZE, Z_SIZE, GH_SIZE, tr_charset, dismodel, MAXLENGTH; dropout=GDROPOUT, layers=2)\n",
    "trnsampler = Sampler(dtrn, tr_charset, genmodel, BATCHSIZE * 2)\n",
    "devsampler = Sampler(ddev, tr_charset, genmodel, BATCHSIZE * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn no:1\n",
      "Ex.Generated words: \n",
      "kŞŞŞŞŞŞŞŞŞŞŞŞŞs\n",
      "PPPPPPPPPPPPPPP\n",
      "İİİİİİİvvvvvvvv\n",
      "nnnnnnnnnnnnnnB\n",
      "vvvvvvvvvvvvvvv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Started training...\n",
      "└ @ Main In[15]:30\n",
      "\n",
      "┣▍                   ┫ [2.00%, 1/50, 00:00/00:06, 7.79i/s] (dev = 131.54707f0, tst = 0.6842695f0, mem = 9.125029f9)\n",
      "┣████████████████████┫ [100.00%, 50/50, 00:00/00:00, 142.65i/s] (dev = 8.062433f0, tst = 0.041713875f0, mem = 9.186932f9)\n",
      "\n",
      "┣▏                   ┫ [0.67%, 1/150, 00:39/01:38:10, 39.26s/i] (dev = 13808.697f0, tst = 70.82898f0, mem = 9.183227f9)\n",
      "┣▌                   ┫ [2.67%, 4/150, 01:44/01:05:04, 21.61s/i] (dev = 12870.244f0, tst = 65.64877f0, mem = 9.24778f9)\n",
      "┣▉                   ┫ [4.67%, 7/150, 02:47/59:35, 20.91s/i] (dev = 10306.964f0, tst = 56.2242f0, mem = 9.244681f9)\n",
      "┣█▎                  ┫ [6.67%, 10/150, 03:50/57:32, 21.10s/i] (dev = 8358.897f0, tst = 60.42759f0, mem = 9.203965f9)\n",
      "┣█▋                  ┫ [8.67%, 13/150, 04:55/56:49, 21.79s/i] (dev = 10132.482f0, tst = 78.33244f0, mem = 8.658175f9)\n",
      "┣██▏                 ┫ [10.67%, 16/150, 05:56/55:40, 20.28s/i] (dev = 9516.701f0, tst = 72.090324f0, mem = 9.293023f8)\n",
      "┣██▌                 ┫ [12.67%, 19/150, 06:57/54:49, 20.11s/i] (dev = 7820.9434f0, tst = 55.699196f0, mem = 8.3699514f8)\n",
      "┣██▉                 ┫ [14.67%, 22/150, 07:59/54:26, 20.78s/i] (dev = 7694.236f0, tst = 49.836647f0, mem = 8.970298f8)\n",
      "┣███▎                ┫ [16.67%, 25/150, 09:03/54:16, 21.24s/i] (dev = 8118.2065f0, tst = 49.545555f0, mem = 8.971042f8)\n",
      "┣███▋                ┫ [18.67%, 28/150, 10:07/54:12, 21.46s/i] (dev = 8229.35f0, tst = 49.437973f0, mem = 8.971042f8)\n",
      "┣████▏               ┫ [20.67%, 31/150, 11:16/54:32, 23.07s/i] (dev = 8056.0693f0, tst = 48.953312f0, mem = 8.973828f8)\n",
      "┣████▌               ┫ [22.67%, 34/150, 12:25/54:48, 23.00s/i] (dev = 7546.9297f0, tst = 48.800293f0, mem = 9.152168f8)\n",
      "┣████▉               ┫ [24.67%, 37/150, 13:32/54:53, 22.30s/i] (dev = 7408.3457f0, tst = 50.865578f0, mem = 9.152168f8)\n",
      "┣█████▎              ┫ [26.67%, 40/150, 14:43/55:10, 23.46s/i] (dev = 7615.66f0, tst = 54.374043f0, mem = 9.152168f8)\n",
      "┣█████▋              ┫ [28.67%, 43/150, 15:50/55:14, 22.44s/i] (dev = 7918.7295f0, tst = 57.810497f0, mem = 9.152168f8)\n",
      "┣██████▏             ┫ [30.67%, 46/150, 16:55/55:09, 21.58s/i] (dev = 7790.2104f0, tst = 56.71481f0, mem = 9.152167f8)\n",
      "┣██████▌             ┫ [32.67%, 49/150, 17:58/55:00, 21.14s/i] (dev = 7322.8022f0, tst = 51.0093f0, mem = 9.151593f8)\n",
      "┣██████▉             ┫ [34.67%, 52/150, 18:59/54:47, 20.48s/i] (dev = 7346.96f0, tst = 47.54151f0, mem = 8.80229f8)\n",
      "┣███████▎            ┫ [36.67%, 55/150, 20:03/54:41, 21.21s/i] (dev = 7467.849f0, tst = 47.153152f0, mem = 8.7944256f8)\n",
      "┣███████▋            ┫ [38.67%, 58/150, 21:07/54:37, 21.27s/i] (dev = 7299.7466f0, tst = 47.61232f0, mem = 8.810072f8)\n",
      "┣████████▎           ┫ [41.33%, 62/150, 22:18/53:56, 17.68s/i] (dev = 7275.2827f0, tst = 47.598938f0, mem = 9.110228f8)\n",
      "┣████████▋           ┫ [43.33%, 65/150, 23:19/53:49, 20.58s/i] (dev = 7336.268f0, tst = 47.07557f0, mem = 9.194114f8)\n",
      "┣█████████           ┫ [45.33%, 68/150, 24:24/53:48, 21.38s/i] (dev = 7293.3374f0, tst = 47.01661f0, mem = 9.19411f8)\n",
      "┣█████████▍          ┫ [47.33%, 71/150, 25:26/53:43, 20.70s/i] (dev = 7162.9453f0, tst = 48.07521f0, mem = 9.194107f8)\n",
      "┣█████████▊          ┫ [49.33%, 74/150, 26:26/53:35, 20.11s/i] (dev = 7235.603f0, tst = 50.99421f0, mem = 9.1833715f8)\n",
      "┣██████████▎         ┫ [51.33%, 77/150, 27:29/53:33, 21.16s/i] (dev = 7339.115f0, tst = 52.665203f0, mem = 9.1780435f8)\n",
      "┣██████████▋         ┫ [53.33%, 80/150, 28:33/53:31, 21.04s/i] (dev = 7296.468f0, tst = 52.31981f0, mem = 8.8079405f8)\n",
      "┣███████████         ┫ [55.33%, 83/150, 29:38/53:34, 21.87s/i] (dev = 7081.117f0, tst = 48.799183f0, mem = 8.595604f8)\n",
      "┣███████████▍        ┫ [57.33%, 86/150, 30:49/53:44, 23.50s/i] (dev = 7272.447f0, tst = 45.313786f0, mem = 9.1467616f8)\n",
      "┣███████████▊        ┫ [59.33%, 89/150, 31:57/53:51, 22.78s/i] (dev = 7416.595f0, tst = 45.07664f0, mem = 8.9789894f8)\n",
      "┣████████████▎       ┫ [61.33%, 92/150, 33:07/53:59, 23.20s/i] (dev = 7277.6167f0, tst = 45.048374f0, mem = 9.1467616f8)\n",
      "┣████████████▋       ┫ [63.33%, 95/150, 34:13/54:02, 22.23s/i] (dev = 7056.2393f0, tst = 48.87895f0, mem = 9.1466784f8)\n",
      "┣█████████████       ┫ [65.33%, 98/150, 35:16/53:58, 20.78s/i] (dev = 7513.497f0, tst = 54.94564f0, mem = 9.146677f8)\n",
      "┣█████████████▍      ┫ [67.33%, 101/150, 36:17/53:53, 20.51s/i] (dev = 7603.26f0, tst = 55.669003f0, mem = 9.1466784f8)\n",
      "┣█████████████▊      ┫ [69.33%, 104/150, 37:19/53:49, 20.48s/i] (dev = 7272.3755f0, tst = 51.74597f0, mem = 9.1466784f8)\n",
      "┣██████████████▎     ┫ [71.33%, 107/150, 38:22/53:48, 21.24s/i] (dev = 7129.1885f0, tst = 49.1819f0, mem = 8.978906f8)\n",
      "┣██████████████▋     ┫ [73.33%, 110/150, 39:24/53:43, 20.49s/i] (dev = 7133.7734f0, tst = 47.709545f0, mem = 8.9733357f8)\n",
      "┣███████████████     ┫ [75.33%, 113/150, 40:26/53:40, 20.75s/i] (dev = 7146.8604f0, tst = 47.278698f0, mem = 8.6359904f8)\n",
      "┣███████████████▍    ┫ [77.33%, 116/150, 41:28/53:37, 20.55s/i] (dev = 7048.9805f0, tst = 47.743286f0, mem = 8.635991f8)\n",
      "┣███████████████▊    ┫ [79.33%, 119/150, 42:29/53:34, 20.59s/i] (dev = 7026.148f0, tst = 48.89895f0, mem = 8.640256f8)\n",
      "┣████████████████▎   ┫ [81.33%, 122/150, 43:32/53:31, 20.80s/i] (dev = 7032.861f0, tst = 49.55101f0, mem = 8.979077f8)\n",
      "┣████████████████▋   ┫ [83.33%, 125/150, 44:36/53:31, 21.29s/i] (dev = 6957.27f0, tst = 48.848667f0, mem = 8.979077f8)\n",
      "┣█████████████████   ┫ [85.33%, 128/150, 45:39/53:30, 21.13s/i] (dev = 6862.9585f0, tst = 47.730503f0, mem = 8.978913f8)\n",
      "┣█████████████████▍  ┫ [87.33%, 131/150, 46:43/53:29, 21.25s/i] (dev = 6787.4917f0, tst = 46.72657f0, mem = 8.978913f8)\n",
      "┣█████████████████▊  ┫ [89.33%, 134/150, 47:48/53:31, 21.83s/i] (dev = 6742.891f0, tst = 45.934277f0, mem = 8.97858f8)\n",
      "┣██████████████████▎ ┫ [91.33%, 137/150, 49:00/53:38, 23.72s/i] (dev = 6691.8433f0, tst = 45.536716f0, mem = 8.984067f8)\n",
      "┣██████████████████▋ ┫ [93.33%, 140/150, 50:09/53:43, 23.01s/i] (dev = 6646.9478f0, tst = 45.174294f0, mem = 9.151838f8)\n",
      "┣███████████████████ ┫ [95.33%, 143/150, 51:17/53:47, 22.77s/i] (dev = 6617.886f0, tst = 44.895386f0, mem = 9.15184f8)\n",
      "┣███████████████████▍┫ [97.33%, 146/150, 52:23/53:49, 21.95s/i] (dev = 6538.538f0, tst = 44.55034f0, mem = 9.15184f8)\n",
      "┣███████████████████▊┫ [99.33%, 149/150, 53:27/53:49, 21.48s/i] (dev = 6477.7397f0, tst = 44.683266f0, mem = 9.152168f8)\n",
      "┣████████████████████┫ [100.00%, 150/150, 54:06/54:06, 21.64s/i] (dev = 6473.6045f0, tst = 44.805313f0, mem = 9.186922f8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn no:2\n",
      "Ex.Generated words: \n",
      "aaaaa\n",
      "aaaa\n",
      "aaaa\n",
      "aaaaa\n",
      "aaaa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "┣▍                   ┫ [2.00%, 1/50, 00:00/00:11, 4.62i/s] (dev = 7.8901467f0, tst = 0.04081942f0, mem = 8.2209434f9)\n",
      "┣████████████████████┫ [100.00%, 50/50, 00:01/00:01, 89.16i/s] (dev = 3.8702023f0, tst = 0.020070868f0, mem = 8.93652f9)\n",
      "\n",
      "┣▏                   ┫ [0.67%, 1/150, 00:39/01:37:49, 39.13s/i] (dev = 6477.8394f0, tst = 31.854073f0, mem = 8.936536f9)\n",
      "┣▌                   ┫ [2.67%, 4/150, 01:41/01:03:15, 20.69s/i] (dev = 6473.292f0, tst = 31.55002f0, mem = 8.936295f9)\n",
      "┣▉                   ┫ [4.67%, 7/150, 02:43/58:10, 20.55s/i] (dev = 6395.141f0, tst = 31.833187f0, mem = 8.936295f9)\n",
      "┣█▎                  ┫ [6.67%, 10/150, 03:42/55:36, 19.84s/i] (dev = 6429.8037f0, tst = 32.1701f0, mem = 8.936433f9)\n",
      "┣█▋                  ┫ [8.67%, 13/150, 04:44/54:36, 20.52s/i] (dev = 6388.814f0, tst = 31.876644f0, mem = 8.712405f9)\n",
      "┣██▏                 ┫ [10.67%, 16/150, 05:48/54:23, 21.37s/i] (dev = 6327.905f0, tst = 31.314232f0, mem = 9.588908f8)\n",
      "┣██▌                 ┫ [12.67%, 19/150, 06:51/54:07, 21.06s/i] (dev = 6316.202f0, tst = 30.937725f0, mem = 9.510429f8)\n",
      "┣██▉                 ┫ [14.67%, 22/150, 07:54/53:50, 20.81s/i] (dev = 6318.918f0, tst = 30.767906f0, mem = 9.5968544f8)\n",
      "┣███▎                ┫ [16.67%, 25/150, 08:57/53:44, 21.24s/i] (dev = 6277.279f0, tst = 31.003529f0, mem = 9.649857f8)\n",
      "┣███▋                ┫ [18.67%, 28/150, 09:59/53:30, 20.61s/i] (dev = 6278.537f0, tst = 31.263117f0, mem = 9.649842f8)\n",
      "┣████▏               ┫ [20.67%, 31/150, 11:02/53:25, 21.04s/i] (dev = 6290.6465f0, tst = 31.439875f0, mem = 9.649855f8)\n",
      "┣████▌               ┫ [22.67%, 34/150, 12:07/53:26, 21.47s/i] (dev = 6283.5347f0, tst = 31.434092f0, mem = 9.649842f8)\n",
      "┣████▉               ┫ [24.67%, 37/150, 13:15/53:42, 22.67s/i] (dev = 6249.861f0, tst = 31.1807f0, mem = 9.6498675f8)\n",
      "┣█████▎              ┫ [26.67%, 40/150, 14:20/53:44, 21.68s/i] (dev = 6205.6064f0, tst = 30.580236f0, mem = 9.61284f8)\n",
      "┣█████▋              ┫ [28.67%, 43/150, 15:27/53:55, 22.51s/i] (dev = 6285.1167f0, tst = 30.29405f0, mem = 9.596865f8)\n",
      "┣██████▏             ┫ [30.67%, 46/150, 16:34/54:00, 22.07s/i] (dev = 6259.5522f0, tst = 30.292332f0, mem = 9.6021677f8)\n",
      "┣██████▌             ┫ [32.67%, 49/150, 17:38/54:00, 21.63s/i] (dev = 6223.343f0, tst = 30.322313f0, mem = 9.601815f8)\n",
      "┣██████▉             ┫ [34.67%, 52/150, 18:42/53:57, 21.28s/i] (dev = 6199.5605f0, tst = 30.270626f0, mem = 9.608044f8)\n",
      "┣███████▎            ┫ [36.67%, 55/150, 19:45/53:53, 21.04s/i] (dev = 6161.018f0, tst = 30.133669f0, mem = 9.608044f8)\n",
      "┣███████▋            ┫ [38.67%, 58/150, 20:49/53:51, 21.29s/i] (dev = 6145.126f0, tst = 30.119884f0, mem = 9.608372f8)\n",
      "┣████████▏           ┫ [40.67%, 61/150, 21:53/53:48, 21.13s/i] (dev = 6143.1606f0, tst = 30.011631f0, mem = 9.6189395f8)\n",
      "┣████████▌           ┫ [42.67%, 64/150, 22:56/53:45, 21.16s/i] (dev = 6138.3477f0, tst = 29.85224f0, mem = 9.6189395f8)\n",
      "┣████████▉           ┫ [44.67%, 67/150, 23:59/53:41, 20.78s/i] (dev = 6182.0063f0, tst = 29.90276f0, mem = 9.618138f8)\n",
      "┣█████████▎          ┫ [46.67%, 70/150, 25:01/53:37, 20.86s/i] (dev = 6100.7695f0, tst = 30.008036f0, mem = 9.6490656f8)\n",
      "┣█████████▋          ┫ [48.67%, 73/150, 26:04/53:34, 21.01s/i] (dev = 6110.6353f0, tst = 30.423283f0, mem = 9.6495136f8)\n",
      "┣██████████▏         ┫ [50.67%, 76/150, 27:08/53:32, 21.13s/i] (dev = 6241.3716f0, tst = 31.326721f0, mem = 9.649842f8)\n",
      "┣██████████▌         ┫ [52.67%, 79/150, 28:12/53:33, 21.63s/i] (dev = 6256.7754f0, tst = 31.401087f0, mem = 9.649842f8)\n",
      "┣██████████▉         ┫ [54.67%, 82/150, 29:21/53:41, 22.78s/i] (dev = 6146.7114f0, tst = 30.787205f0, mem = 9.649842f8)\n",
      "┣███████████▍        ┫ [57.33%, 86/150, 30:30/53:12, 17.33s/i] (dev = 6061.487f0, tst = 30.077854f0, mem = 9.6604506f8)\n",
      "┣███████████▊        ┫ [59.33%, 89/150, 31:37/53:17, 22.18s/i] (dev = 6113.366f0, tst = 30.00885f0, mem = 9.6604506f8)\n",
      "┣████████████▎       ┫ [61.33%, 92/150, 32:43/53:21, 22.11s/i] (dev = 6072.975f0, tst = 30.00493f0, mem = 9.660124f8)\n",
      "┣████████████▋       ┫ [63.33%, 95/150, 33:48/53:22, 21.61s/i] (dev = 6081.5767f0, tst = 30.468895f0, mem = 9.65996f8)\n",
      "┣█████████████       ┫ [65.33%, 98/150, 34:51/53:21, 21.20s/i] (dev = 6137.1704f0, tst = 30.800709f0, mem = 9.660454f8)\n",
      "┣█████████████▌      ┫ [68.00%, 102/150, 36:03/53:01, 17.85s/i] (dev = 6099.016f0, tst = 30.489748f0, mem = 9.840546f8)\n",
      "┣██████████████      ┫ [70.00%, 105/150, 37:07/53:01, 21.34s/i] (dev = 6047.769f0, tst = 30.074707f0, mem = 9.5023795f8)\n",
      "┣██████████████▍     ┫ [72.00%, 108/150, 38:10/53:00, 21.00s/i] (dev = 6014.158f0, tst = 29.621515f0, mem = 9.75666f8)\n",
      "┣██████████████▊     ┫ [74.00%, 111/150, 39:13/53:00, 21.10s/i] (dev = 5981.3647f0, tst = 29.307074f0, mem = 9.75666f8)\n",
      "┣███████████████▏    ┫ [76.00%, 114/150, 40:16/53:00, 21.13s/i] (dev = 5963.6353f0, tst = 29.21644f0, mem = 9.75666f8)\n",
      "┣███████████████▌    ┫ [78.00%, 117/150, 41:21/53:00, 21.35s/i] (dev = 5953.932f0, tst = 29.07607f0, mem = 9.60217f8)\n",
      "┣████████████████    ┫ [80.00%, 120/150, 42:25/53:02, 21.61s/i] (dev = 5952.6484f0, tst = 28.894266f0, mem = 9.5595174f8)\n",
      "┣████████████████▍   ┫ [82.00%, 123/150, 43:29/53:01, 21.15s/i] (dev = 5901.3003f0, tst = 28.699162f0, mem = 9.591595f8)\n",
      "┣████████████████▊   ┫ [84.00%, 126/150, 44:31/53:00, 20.88s/i] (dev = 5877.032f0, tst = 28.681906f0, mem = 9.591596f8)\n",
      "┣█████████████████▏  ┫ [86.00%, 129/150, 45:34/53:00, 21.01s/i] (dev = 5880.04f0, tst = 29.00623f0, mem = 9.591596f8)\n",
      "┣█████████████████▌  ┫ [88.00%, 132/150, 46:37/52:59, 20.93s/i] (dev = 5909.1187f0, tst = 29.513445f0, mem = 9.591936f8)\n",
      "┣██████████████████  ┫ [90.00%, 135/150, 47:47/53:06, 23.25s/i] (dev = 5843.502f0, tst = 29.578876f0, mem = 9.597179f8)\n",
      "┣██████████████████▍ ┫ [92.00%, 138/150, 48:56/53:11, 23.00s/i] (dev = 5776.1597f0, tst = 29.163536f0, mem = 9.6079104f8)\n",
      "┣██████████████████▊ ┫ [94.00%, 141/150, 50:05/53:17, 22.90s/i] (dev = 5832.3823f0, tst = 29.094511f0, mem = 9.6079104f8)\n",
      "┣███████████████████▏┫ [96.00%, 144/150, 51:09/53:17, 21.46s/i] (dev = 5869.9f0, tst = 29.086369f0, mem = 9.6079104f8)\n",
      "┣███████████████████▌┫ [98.00%, 147/150, 52:13/53:17, 21.35s/i] (dev = 5693.5273f0, tst = 28.539726f0, mem = 9.607569f8)\n",
      "┣████████████████████┫ [100.00%, 150/150, 53:17/53:17, 21.21s/i] (dev = 5663.295f0, tst = 28.557549f0, mem = 9.607899f8)\n",
      "┣████████████████████┫ [100.00%, 150/150, 53:46/53:46, 21.51s/i] (dev = 5663.295f0, tst = 28.557549f0, mem = 9.586538f8)\n"
     ]
    }
   ],
   "source": [
    "ctrn = collect(dtrn)\n",
    "cdev = collect(ddev)\n",
    "collecttrn = [ ((arraybatch(dtrn, i), get_z(Z_SIZE, size(i, 1), 1)), 1) for i in ctrn ]\n",
    "collectdev = [ ((arraybatch(ddev, i), get_z(Z_SIZE, size(i, 1), 1)), 1) for i in cdev ]\n",
    "\n",
    "function gmodel(batches)\n",
    "    global genmodel\n",
    "    global collecttrn\n",
    "    global collectdev\n",
    "    \n",
    "    trnxbatches = shuffle!(collecttrn)[1:batches]\n",
    "    devbatches = shuffle!(collectdev)\n",
    "    trnmini = trnxbatches[1:1]\n",
    "\n",
    "    genmodel = train!(genmodel, params(genmodel)[1:3], trnxbatches, devbatches, trnmini)\n",
    "end\n",
    "\n",
    "function dmodel(batches)\n",
    "    global trnsampler\n",
    "    global devsampler\n",
    "    global dismodel\n",
    "    \n",
    "    ctrn = collect(trnsampler)\n",
    "    ctrn = shuffle!(ctrn)[1:batches]\n",
    "    trnmini = ctrn[1:1]\n",
    "    dev = collect(devsampler)\n",
    "    dismodel = train!(dismodel, params(dismodel), ctrn, dev, trnmini) \n",
    "end\n",
    "\n",
    "@info \"Started training...\"\n",
    "for k in 1:2\n",
    "    println(\"Turn no:\", k)\n",
    "    println(\"Ex.Generated words: \\n\", join([ generate(genmodel; maxlength=MAXLENGTH) for i in 1:5 ],\"\\n\"))\n",
    "    dmodel(50)\n",
    "    gmodel(150)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex.Generated words: \n",
      "karatını\n",
      "karatılarını\n",
      "karanlarını\n",
      "karatını\n",
      "karatını\n",
      "karatılarını\n",
      "karatını\n",
      "karılarını\n",
      "kararını\n",
      "kararının\n",
      "karatını\n",
      "karatını\n",
      "karanlarını\n",
      "karatılarını\n",
      "karanlarını\n",
      "karatının\n",
      "karılarını\n",
      "karatının\n",
      "karılarını\n",
      "karatılarını\n",
      "karılarını\n",
      "karılarını\n",
      "karatılarını\n",
      "kararını\n",
      "karatılarını\n",
      "karatılarını\n",
      "karatıların\n",
      "kararını\n",
      "karatının\n",
      "kararını\n",
      "karatını\n",
      "karatını\n",
      "karatılarını\n",
      "karatının\n",
      "karanlarını\n",
      "karılarını\n",
      "karatını\n",
      "karatının\n",
      "karatını\n",
      "karılarını\n",
      "karatılarını\n",
      "karılmasını\n",
      "karatılarını\n",
      "karatıların\n",
      "karatını\n",
      "karatılarını\n",
      "karatılarını\n",
      "karatını\n",
      "karatını\n",
      "karatını\n",
      "karatının\n",
      "karatılarını\n",
      "karılarını\n",
      "karanlarını\n",
      "karatını\n",
      "karatının\n",
      "karatını\n",
      "karatını\n",
      "karılarını\n",
      "karatını\n",
      "karatılarını\n",
      "karatını\n",
      "karatılarını\n",
      "karatını\n",
      "karatılarını\n",
      "karatını\n",
      "karanlarını\n",
      "karatını\n",
      "karatılarını\n",
      "karatılarını\n",
      "karılarını\n",
      "karanlarını\n",
      "karatılarını\n",
      "karılarını\n",
      "karatını\n",
      "karatılarını\n",
      "karılarını\n",
      "kararının\n",
      "karatını\n",
      "karatını\n",
      "karatını\n",
      "karanlarını\n",
      "karatının\n",
      "karatıların\n",
      "karatının\n",
      "kararını\n",
      "karanlarını\n",
      "karatılarını\n",
      "kararını\n",
      "karatının\n",
      "karatını\n",
      "karatını\n",
      "karatılarını\n",
      "karatını\n",
      "karatılarını\n",
      "karatılarını\n",
      "karatılarını\n",
      "karatını\n",
      "karatılarını\n",
      "karatını\n"
     ]
    }
   ],
   "source": [
    "println(\"Ex.Generated words: \\n\", join([ generate(genmodel; maxlength=MAXLENGTH, start=\"\") for i in 1:100 ],\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "Knet.save(\"genmodel.jld2\", \"genmodel\", genmodel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
