{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-GAN Turkish word generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readwordset (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run(`curl -o asafaya.omutlu.jld2 \"https://doc-0g-70-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qc1fmd13iog7eno06jm2f54mtssrc19l/1574712000000/14279118924456930669/*/1GNdxziPk0mK4tcWiCecPnvSH-jBEX9tk?e=download\"`)\n",
    "\n",
    "using Knet, Test, Base.Iterators, Printf, LinearAlgebra, CuArrays, Random, IterTools\n",
    "\n",
    "struct Vocab\n",
    "    w2i::Dict{String,Int}\n",
    "    i2w::Vector{String}\n",
    "    unk::Int\n",
    "    eos::Int\n",
    "    tokenizer\n",
    "end\n",
    "\n",
    "function Vocab(file::String; tokenizer=split, vocabsize=Inf, mincount=1, unk=\"<unk>\", eos=\"<s>\")\n",
    "    # set unk and eos tokens frequency to inf because\n",
    "    # we don't want them to be removed from the vocab set\n",
    "    cdict = Dict(eos => Inf, unk=>Inf) \n",
    "    \n",
    "    # create vocab set and count occurrences\n",
    "    for l in eachline(file)\n",
    "        tokens = tokenizer(l)\n",
    "        map(w -> cdict[w] = get!(cdict, w, 0) + 1, tokens)\n",
    "    end\n",
    "    \n",
    "    # select words with frequency higher than mincount\n",
    "    # sort by frequency and delete if vocabsize is determined\n",
    "    fsorted = sort([ (w, c) for (w, c) in cdict if c >= mincount ], by = x -> x[2], rev = true)\n",
    "    \n",
    "    vocabsize == Inf || (fsorted = fsorted[1:vocabsize])\n",
    "\n",
    "    i2w = [ eos; unk; [ x[1] for x in fsorted[3:end] ] ]\n",
    "    w2i = Dict( w => i for (i, w) in enumerate(i2w))                \n",
    "    \n",
    "    return Vocab(w2i, i2w, w2i[unk], w2i[eos], tokenizer)\n",
    "end\n",
    "                \n",
    "struct TextReader\n",
    "    file::String\n",
    "    vocab::Vocab\n",
    "end\n",
    "                \n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    s === nothing && (s = open(r.file))\n",
    "    eof(s) && return close(s)\n",
    "    return [ get(r.vocab.w2i, w, r.vocab.unk) for w in r.vocab.tokenizer(readline(s))], s\n",
    "end\n",
    "                \n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}\n",
    "\n",
    "struct WordsData\n",
    "    src::TextReader        \n",
    "    batchsize::Int         \n",
    "    maxlength::Int         \n",
    "    batchmajor::Bool       \n",
    "    bucketwidth::Int    \n",
    "    buckets::Vector        \n",
    "end\n",
    "\n",
    "function WordsData(src::TextReader; batchsize = 128, maxlength = typemax(Int),\n",
    "                batchmajor = false, bucketwidth = 2, numbuckets = min(128, maxlength ÷ bucketwidth))\n",
    "    buckets = [ [] for i in 1:numbuckets ] # buckets[i] is an array of sentence pairs with similar length\n",
    "    WordsData(src, batchsize, maxlength, batchmajor, bucketwidth, buckets)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{WordsData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{WordsData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{WordsData}) = NTuple{2}\n",
    "\n",
    "function Base.iterate(d::WordsData, state=nothing)\n",
    "    if state == 0 # When file is finished but buckets are partially full \n",
    "        for i in 1:length(d.buckets)\n",
    "            if length(d.buckets[i]) > 0\n",
    "                buc = d.buckets[i]\n",
    "                d.buckets[i] = []\n",
    "                return buc, state\n",
    "            end\n",
    "        end\n",
    "        return nothing # Finish iteration\n",
    "    end\n",
    "\n",
    "    while true\n",
    "        src_next = iterate(d.src, state)\n",
    "        \n",
    "        if src_next === nothing\n",
    "            state = 0\n",
    "            return iterate(d, state)\n",
    "        end\n",
    "        \n",
    "        (src_word, src_state) = src_next\n",
    "        state = src_state\n",
    "        src_length = length(src_word)\n",
    "        \n",
    "        (src_length > d.maxlength) && continue\n",
    "\n",
    "        i = Int(ceil(src_length / d.bucketwidth))\n",
    "        i > length(d.buckets) && (i = length(d.buckets))\n",
    "\n",
    "        push!(d.buckets[i], src_word)\n",
    "        if length(d.buckets[i]) == d.batchsize\n",
    "            buc = d.buckets[i]\n",
    "            d.buckets[i] = []\n",
    "            return buc, state\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function readwordset(fname)\n",
    "    words = []\n",
    "    fi = open(fname)\n",
    "    while !eof(fi)\n",
    "        push!(words, readline(fi))\n",
    "    end\n",
    "    close(fi)\n",
    "    words\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G/D Common Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Embed; w; end\n",
    "\n",
    "function Embed(shape...)\n",
    "    Embed(param(shape...))\n",
    "end\n",
    "\n",
    "# per-word loss (in this case per-batch loss)\n",
    "function loss(model, data; average=true)\n",
    "    l = 0\n",
    "    n = 0\n",
    "    a = 0\n",
    "    for (x, y) in data\n",
    "        v = model(x, y; average=false)\n",
    "        l += v[1]\n",
    "        n += v[2]\n",
    "        a += (v[1] / v[2])\n",
    "    end\n",
    "    average && return a\n",
    "    return l, n\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (l::Embed)(x)\n",
    "    dims = size(x)\n",
    "    em = l.w * reshape(x, dims[1], dims[2] * dims[3]) # reshape for multiplication \n",
    "    em = reshape(em, size(em, 1), dims[2], dims[3]) # reshape to original size\n",
    "end\n",
    "\n",
    "struct Dense; w; b; f; p; end\n",
    "(d::Dense)(x) = d.f.(d.w * mat(dropout(x,d.p)) .+ d.b) # mat reshapes 3-D tensor to 2-D matrix so we can use matmul\n",
    "Dense(i::Int,o::Int,f=relu;pdrop=0) = Dense(param(o,i), param0(o), f, pdrop)\n",
    "\n",
    "mutable struct DisModel\n",
    "    vocab::Vocab\n",
    "    embed::Embed\n",
    "    rnn::RNN\n",
    "    denselayers\n",
    "end\n",
    "\n",
    "# This discriminator uses separate weights for its embedding layer\n",
    "function DisModel(vocab, embeddingSize::Int, hidden, denselayers; layers=1, dropout=0)\n",
    "    Em = Embed(embeddingSize, length(vocab.w2i))\n",
    "    rnn = RNN(embeddingSize, hidden; numLayers=layers, dropout=dropout)\n",
    "    DisModel(vocab, Em, rnn, denselayers)\n",
    "end\n",
    "\n",
    "function (c::DisModel)(x) # the input here is weights of the characters with shape (C, B, T)\n",
    "    c.rnn.h, c.rnn.c = 0, 0\n",
    "    em = c.embed(x)\n",
    "    rnn_out = permutedims(c.rnn(em), [1, 3, 2])\n",
    "    for l in c.denselayers\n",
    "        rnn_out = l(rnn_out)\n",
    "    end\n",
    "    rnn_out\n",
    "end\n",
    "\n",
    "(c::DisModel)(x,y; average=true) = nll(c(x), y; average=average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_z(shape...) = KnetArray(randn(Float32, shape...))\n",
    "\n",
    "### Not used \n",
    "# concatenate z with embedding vectors, z -> (z_size, B), returns (E+z_size, B, T)\n",
    "# this will be used to feed Z to generator at each timestep\n",
    "# function (l::Embed)(x, z)\n",
    "#     em = l.w[:, x]\n",
    "#     z_array = cat((z for i in 1:size(em, 3))...; dims=(3))\n",
    "#     cat(em, z_array; dims=(1))\n",
    "# end\n",
    "\n",
    "# Generator model\n",
    "struct GenModel\n",
    "    projection::Embed\n",
    "    rnn::RNN        \n",
    "    dropout::Real\n",
    "    vocab::Vocab\n",
    "    disModel::DisModel\n",
    "    maxlength::Int\n",
    "end\n",
    "\n",
    "function GenModel(inputsize::Int, hidden::Int, vocab::Vocab, disModel::DisModel, maxlength::Int; layers=2, dropout=0)\n",
    "    rnn = RNN(inputsize, hidden; numLayers=layers, dropout=dropout)\n",
    "    projection = Embed(hidden, length(vocab.i2w))\n",
    "    GenModel(projection, rnn, dropout, vocab, disModel, maxlength)\n",
    "end\n",
    "\n",
    "# This generator shares the projection layers weights of the discriminator for its projection layer\n",
    "function GenModel(inputsize::Int, vocab::Vocab, disModel::DisModel, maxlength::Int; layers=2, dropout=0)\n",
    "    rnn = RNN(inputsize, size(disModel.embed.w, 1); numLayers=layers, dropout=dropout)\n",
    "    GenModel(disModel.embed, rnn, dropout, vocab, disModel, maxlength)\n",
    "end\n",
    "\n",
    "function Z(s::GenModel, batchsize, timesteps)\n",
    "    z = get_z(s.rnn.inputSize, batchsize, 1) # according to get_z(H, B, layers)\n",
    "    return cat([ z for i in 1:timesteps]...;dims=3)\n",
    "end\n",
    "\n",
    "# Generator forward pass, size(Z) -> inputsize, batchsize, sequencelength\n",
    "function (s::GenModel)(Z)\n",
    "    s.rnn.h, s.rnn.c = 0, 0\n",
    "    rnn_out = s.rnn(Z) \n",
    "    dims = size(rnn_out)\n",
    "    output = s.projection.w' * dropout(reshape(rnn_out, dims[1], dims[2] * dims[3]), s.dropout)\n",
    "    reshape(softmax(output), size(output, 1), dims[2], dims[3])\n",
    "end\n",
    "\n",
    "# Generator loss\n",
    "function (s::GenModel)(Z, calculateloss::Int; average=true)\n",
    "    y = Array(ones(Int, size(Z, 2))) # create labels 1 -> real, 2-> not-real\n",
    "    x = s(Z)\n",
    "    pads = KnetArray(zeros(Float32, size(x, 1), size(x, 2), s.maxlength - size(x, 3)))\n",
    "    pads[s.vocab.eos, :, :] .= 1\n",
    "    x = cat(x, pads; dims=3) # padding\n",
    "    return s.disModel(x, y;average=average) \n",
    "end\n",
    "\n",
    "function generate(s::GenModel, maxlength, batchsize)\n",
    "    out = s(Z(s, batchsize, maxlength))\n",
    "    words = []\n",
    "    for i in 1:batchsize\n",
    "        push!(words, join([s.vocab.i2w[x[1]] for x in argmax(out[:, i, :]; dims=1)], \" \"))\n",
    "    end\n",
    "    words\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Sampler\n",
    "    wordsdata::WordsData\n",
    "    vocab::Vocab\n",
    "    genModel::GenModel\n",
    "    maxBatchsize::Int\n",
    "end\n",
    "\n",
    "# this function is similar to gumble softmax, it is used to soften the one-hot-vector of the real samples\n",
    "# tau -> normalization factor; the bigger the softer\n",
    "function soften(A; dims=1, tau=2.0) \n",
    "    A = A ./ tau\n",
    "    softmax(A; dims=dims)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{Sampler}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{Sampler}) = Base.HasEltype()\n",
    "Base.eltype(::Type{Sampler}) = Tuple{KnetArray{Float32,3},Array{Int64,1}}\n",
    "\n",
    "function Base.iterate(s::Sampler, state=nothing)\n",
    "    wdatastate = iterate(s.wordsdata, state)\n",
    "    wdatastate === nothing && (return nothing)\n",
    "    \n",
    "    (bucket, state) = wdatastate\n",
    "    bsize = length(bucket)\n",
    "    src_eow = s.vocab.eos\n",
    "    src_lengths = map(x -> length(x), bucket)\n",
    "    max_length = max(src_lengths...)\n",
    "    gsize = 1 + rand(bsize:s.maxBatchsize) - bsize # count of words to be generated\n",
    "    generated = s.genModel(Z(s.genModel, gsize, max_length))\n",
    "\n",
    "    to_be_cat = [generated, ]\n",
    "    for (i, v) in enumerate(bucket)\n",
    "        tindex = [i for i in 1:length(v)]\n",
    "        onehot = KnetArray(zeros(Float32, length(s.vocab.w2i), 1, max_length))\n",
    "        onehot[v, :, tindex] .= 1\n",
    "        onehot = soften(onehot) # soften one hot vectors elements value\n",
    "        push!(to_be_cat, onehot)\n",
    "    end\n",
    "    x = cat(to_be_cat...;dims=2) # concatenate both generated and sampled words\n",
    "    pads = KnetArray(zeros(Float32, size(x, 1), size(x, 2), s.wordsdata.maxlength - size(x, 3)))\n",
    "    pads[src_eow, :, :] .= 1\n",
    "    \n",
    "#     pads = soften(pads) # getting error \n",
    "\n",
    "    x = cat(x, pads; dims=3) # padding\n",
    "    y = Array(ones(Int, gsize+bsize)) # create labels 1 -> real, 2-> not-real\n",
    "    y[1:gsize] = y[1:gsize] .+ 1\n",
    "    \n",
    "    ind = shuffle(1:gsize+bsize) # used to shuffle the batch\n",
    "    x, y = x[:, ind, :], y[ind]\n",
    "    return (x,y), state\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train!(model, parameters, trn, dev, tst...)\n",
    "    bestmodel, bestloss = deepcopy(model), loss(model, dev)\n",
    "    progress!(adam(model, trn; params=parameters), seconds=30) do y\n",
    "        devloss = loss(model, dev)\n",
    "        tstloss = map(d->loss(model,d), tst)\n",
    "        if devloss < bestloss\n",
    "            bestloss, bestmodel = devloss, deepcopy(model)\n",
    "        end\n",
    "        println(stderr)\n",
    "        (dev=devloss, tst=tstloss, mem=Float32(CuArrays.usage[]))\n",
    "    end\n",
    "    return bestmodel\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per-word loss (in this case per-batch loss)\n",
    "function loss(model, data; average=true)\n",
    "    l = 0\n",
    "    n = 0\n",
    "    a = 0\n",
    "    for (x, y) in data\n",
    "        v = model(x, y; average=false)\n",
    "        l += v[1]\n",
    "        n += v[2]\n",
    "        a += (v[1] / v[2])\n",
    "    end\n",
    "    average && return a\n",
    "    return l, n\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 7700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WordsData(TextReader(\"turkish_text/tr.dev\", Vocab(Dict(\"dev\" => 1277,\"metan\" => 5735,\"yüzeyi\" => 4051,\"görüntüleri\" => 3122,\"yaşından\" => 4777,\"yüzeyinde\" => 5042,\"birçoğu\" => 2867,\"geçerlidir\" => 6612,\"2009'da\" => 6885,\"kenar\" => 5186…), [\"<s>\", \"<unk>\", \".\", \",\", \"bir\", \"ve\", \"bu\", \"''\", \"``\", \"için\"  …  \"tamamının\", \"enstrüman\", \"yapmamızı\", \"hayranlık\", \"koruyacak\", \"sebebiyet\", \"izleyiciler\", \"köye\", \"ilişkilerini\", \"silahların\"], 2, 1, split)), 32, 10, false, 1, Array{Any,1}[[], [], [], [], [], [], [], [], [], []])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir = \"turkish_text\"\n",
    "\n",
    "BATCHSIZE = 32\n",
    "MAXLENGTH = 10\n",
    "\n",
    "tr_vocab = Vocab(\"$datadir/tr.train\", mincount=30)\n",
    "println(\"Vocab size \", length(tr_vocab.i2w))\n",
    "tr_trn = TextReader(\"$datadir/tr.train\", tr_vocab)\n",
    "tr_dev = TextReader(\"$datadir/tr.dev\", tr_vocab)\n",
    "dtrn = WordsData(tr_trn, batchsize=BATCHSIZE, maxlength=MAXLENGTH, bucketwidth = 1)\n",
    "ddev = WordsData(tr_dev, batchsize=BATCHSIZE, maxlength=MAXLENGTH, bucketwidth = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sampler(WordsData(TextReader(\"turkish_text/tr.dev\", Vocab(Dict(\"dev\" => 1277,\"metan\" => 5735,\"yüzeyi\" => 4051,\"görüntüleri\" => 3122,\"yaşından\" => 4777,\"yüzeyinde\" => 5042,\"birçoğu\" => 2867,\"geçerlidir\" => 6612,\"2009'da\" => 6885,\"kenar\" => 5186…), [\"<s>\", \"<unk>\", \".\", \",\", \"bir\", \"ve\", \"bu\", \"''\", \"``\", \"için\"  …  \"tamamının\", \"enstrüman\", \"yapmamızı\", \"hayranlık\", \"koruyacak\", \"sebebiyet\", \"izleyiciler\", \"köye\", \"ilişkilerini\", \"silahların\"], 2, 1, split)), 32, 10, false, 1, Array{Any,1}[[], [], [], [], [], [], [], [], [], []]), Vocab(Dict(\"dev\" => 1277,\"metan\" => 5735,\"yüzeyi\" => 4051,\"görüntüleri\" => 3122,\"yaşından\" => 4777,\"yüzeyinde\" => 5042,\"birçoğu\" => 2867,\"geçerlidir\" => 6612,\"2009'da\" => 6885,\"kenar\" => 5186…), [\"<s>\", \"<unk>\", \".\", \",\", \"bir\", \"ve\", \"bu\", \"''\", \"``\", \"için\"  …  \"tamamının\", \"enstrüman\", \"yapmamızı\", \"hayranlık\", \"koruyacak\", \"sebebiyet\", \"izleyiciler\", \"köye\", \"ilişkilerini\", \"silahların\"], 2, 1, split), GenModel(Embed(P(KnetArray{Float32,2}(256,7700))), LSTM(input=128,hidden=256,layers=2,dropout=0.2), 0.2, Vocab(Dict(\"dev\" => 1277,\"metan\" => 5735,\"yüzeyi\" => 4051,\"görüntüleri\" => 3122,\"yaşından\" => 4777,\"yüzeyinde\" => 5042,\"birçoğu\" => 2867,\"geçerlidir\" => 6612,\"2009'da\" => 6885,\"kenar\" => 5186…), [\"<s>\", \"<unk>\", \".\", \",\", \"bir\", \"ve\", \"bu\", \"''\", \"``\", \"için\"  …  \"tamamının\", \"enstrüman\", \"yapmamızı\", \"hayranlık\", \"koruyacak\", \"sebebiyet\", \"izleyiciler\", \"köye\", \"ilişkilerini\", \"silahların\"], 2, 1, split), DisModel(Vocab(Dict(\"dev\" => 1277,\"metan\" => 5735,\"yüzeyi\" => 4051,\"görüntüleri\" => 3122,\"yaşından\" => 4777,\"yüzeyinde\" => 5042,\"birçoğu\" => 2867,\"geçerlidir\" => 6612,\"2009'da\" => 6885,\"kenar\" => 5186…), [\"<s>\", \"<unk>\", \".\", \",\", \"bir\", \"ve\", \"bu\", \"''\", \"``\", \"için\"  …  \"tamamının\", \"enstrüman\", \"yapmamızı\", \"hayranlık\", \"koruyacak\", \"sebebiyet\", \"izleyiciler\", \"köye\", \"ilişkilerini\", \"silahların\"], 2, 1, split), Embed(P(KnetArray{Float32,2}(128,7700))), LSTM(input=128,hidden=256,dropout=0.2), (Dense(P(KnetArray{Float32,2}(16,2560)), P(KnetArray{Float32,1}(16)), NNlib.relu, 0.2), Dense(P(KnetArray{Float32,2}(2,16)), P(KnetArray{Float32,1}(2)), Knet.sigm, 0.2))), 10), 64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "DHIDDEN_SIZE = 256\n",
    "GDROPOUT = 0.2\n",
    "DDROPOUT = 0.2\n",
    "\n",
    "dismodel = DisModel(tr_vocab, EMBEDDING_SIZE, DHIDDEN_SIZE,(\n",
    "        Dense(DHIDDEN_SIZE * MAXLENGTH, 16, pdrop=DDROPOUT),\n",
    "        Dense(16, 2, sigm, pdrop=0.2)\n",
    "        ); dropout=DDROPOUT)\n",
    "\n",
    "GE_SIZE = 256\n",
    "Z_SIZE = 128\n",
    "\n",
    "genmodel = GenModel(Z_SIZE, GE_SIZE, tr_vocab, dismodel, MAXLENGTH; dropout=GDROPOUT, layers=2)\n",
    "trnsampler = Sampler(dtrn, tr_vocab, genmodel, BATCHSIZE * 2)\n",
    "devsampler = Sampler(ddev, tr_vocab, genmodel, BATCHSIZE * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Started training...\n",
      "└ @ Main In[10]:25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn no:1\n",
      "Ex.Generated words: özgür özgür özgür özgür özgür özgür özgür özgür özgür özgür\n",
      "sarı sarı sarı sarı sarı sarı sarı yazmayı yazmayı bozucu\n",
      "tarihi tarihi artmaya artmaya artmaya artmaya artmaya artmaya artmaya artmaya\n",
      "cıvık cıvık dersleri dersleri dersleri dersleri dersleri hücreleri hücreleri hücreleri\n",
      "meme meme okuduğum okuduğum okuduğum okuduğum olurlar olurlar olurlar olurlar\n",
      "Training Discriminator:\n",
      "Effective GPU memory usage: 99.95% (31.704 GiB/31.719 GiB)\n",
      "CuArrays GPU memory usage: 30.967 GiB\n",
      "BinnedPool usage: 30.967 GiB (30.967 GiB allocated, 0 bytes cached)\n",
      "BinnedPool efficiency: 67.24% (20.822 GiB requested, 30.967 GiB allocated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Error: Out of GPU memory trying to allocate 17.624 MiB\n",
      "└ @ CuArrays /kuacc/users/asafaya19/.julia/packages/CuArrays/4ZX56/src/memory.jl:125\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "OutOfMemoryError()",
     "output_type": "error",
     "traceback": [
      "OutOfMemoryError()",
      "",
      "Stacktrace:",
      " [1] alloc at /kuacc/users/asafaya19/.julia/packages/CuArrays/4ZX56/src/memory.jl:128 [inlined]",
      " [2] CuArray{UInt8,1,P} where P(::UndefInitializer, ::Tuple{Int64}) at /kuacc/users/asafaya19/.julia/packages/CuArrays/4ZX56/src/array.jl:90",
      " [3] Type at /kuacc/users/asafaya19/.julia/packages/CuArrays/4ZX56/src/array.jl:98 [inlined]",
      " [4] Type at /kuacc/users/asafaya19/.julia/packages/CuArrays/4ZX56/src/array.jl:99 [inlined]",
      " [5] KnetPtrCu(::Int64) at /kuacc/users/asafaya19/.julia/packages/Knet/HRYiN/src/cuarray.jl:90",
      " [6] Type at /kuacc/users/asafaya19/.julia/packages/Knet/HRYiN/src/kptr.jl:102 [inlined]",
      " [7] KnetArray{Float32,N} where N(::UndefInitializer, ::Tuple{Int64,Int64,Int64}) at /kuacc/users/asafaya19/.julia/packages/Knet/HRYiN/src/karray.jl:82",
      " [8] similar(::KnetArray{Float32,3}, ::Type, ::Tuple{Int64,Int64,Int64}) at /kuacc/users/asafaya19/.julia/packages/Knet/HRYiN/src/karray.jl:164",
      " [9] #cat#48(::Int64, ::typeof(cat), ::KnetArray{Float32,3}, ::Vararg{KnetArray{Float32,3},N} where N) at /kuacc/users/asafaya19/.julia/packages/Knet/HRYiN/src/cuarray.jl:67",
      " [10] (::getfield(Base, Symbol(\"#kw##cat\")))(::NamedTuple{(:dims,),Tuple{Int64}}, ::typeof(cat), ::KnetArray{Float32,3}, ::Vararg{KnetArray{Float32,3},N} where N) at ./none:0",
      " [11] iterate(::Sampler, ::IOStream) at ./In[5]:45",
      " [12] _collect(::UnitRange{Int64}, ::Sampler, ::Base.HasEltype, ::Base.SizeUnknown) at ./array.jl:556",
      " [13] collect(::Sampler) at ./array.jl:544",
      " [14] dmodel(::Int64) at ./In[10]:18",
      " [15] top-level scope at ./In[10]:31"
     ]
    }
   ],
   "source": [
    "function gmodel(epochs)\n",
    "    global genmodel\n",
    "    global BATCHSIZE\n",
    "    global MAXLENGTH\n",
    "    \n",
    "    ctrn = [ (Z(genmodel, BATCHSIZE, MAXLENGTH), 1) for i in 1:500 ]\n",
    "    trnxepoch = collect(flatten(shuffle!(ctrn) for i in 1:epochs))\n",
    "    trnmini = ctrn[1:20]\n",
    "    dev = [ (Z(genmodel, BATCHSIZE, MAXLENGTH), 1) for i in 1:100 ]\n",
    "    genmodel = train!(genmodel, params(genmodel)[1:2], trnxepoch, dev, trnmini)\n",
    "end\n",
    "\n",
    "function dmodel(batches)\n",
    "    global trnsampler\n",
    "    global devsampler\n",
    "    global dismodel\n",
    "    \n",
    "    ctrn = collect(trnsampler)\n",
    "    trnmini = ctrn[1:20]\n",
    "    ctrn = shuffle!(ctrn)[1:batches]\n",
    "    dev = collect(devsampler)\n",
    "    dismodel = train!(dismodel, params(dismodel), ctrn, dev, trnmini) \n",
    "end\n",
    "\n",
    "@info \"Started training...\"\n",
    "for k in 1:20\n",
    "    println(\"Turn no:\", k)\n",
    "    println(\"Ex.Generated words: \", join(generate(genmodel, MAXLENGTH, 5),\"\\n\"))\n",
    "\n",
    "    println(\"Training Discriminator:\")\n",
    "    dmodel(Int(ceil(rand() * 30)))\n",
    "    println(\"Training Generator:\")\n",
    "    gmodel(Int(ceil(rand() * 10)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex.Generated words: lPjjjjj\n",
      "sssssss\n",
      "ssssss\n",
      "ssssss\n",
      "ssssss\n"
     ]
    }
   ],
   "source": [
    "println(\"Ex.Generated words: \", join(generate(genmodel, MAXLENGTH, 5),\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
